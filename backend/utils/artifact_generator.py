"""
Generate artifacts: Jupyter notebook, pickle file, charts, README
"""

import json
import pickle
import base64
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import numpy as np


def generate_notebook(
    df: pd.DataFrame,
    target: str,
    task: str,
    config: Dict[str, Any],
    metrics: Dict[str, float],
    feature_importance: Optional[Dict[str, float]] = None,
    model: Any = None
) -> str:
    """
    Generate a Jupyter notebook with complete training code.
    """
    automl_plan = (config or {}).get("automl_plan") or {}
    # Expect markdown sections in automl_plan if present
    md_sections = []
    if isinstance(automl_plan, dict) and automl_plan:
        # Always show sections if they exist (even if fallback text)
        planning_source = automl_plan.get("planning_source", "unknown")
        md_sections = [
            ("STEP 0 ‚Äî TASK INFERENCE", automl_plan.get("task_inference_md", "") or f"Rule-based fallback task inference (LLM unavailable)."),
            ("STEP 1 ‚Äî DATASET INTELLIGENCE", automl_plan.get("dataset_intelligence_md", "") or f"Rule-based fallback dataset intelligence (LLM unavailable)."),
            ("STEP 2 ‚Äî TRANSFORMATION STRATEGY", automl_plan.get("transformation_strategy_md", "") or f"Rule-based fallback transformation strategy (LLM unavailable)."),
            ("STEP 3 ‚Äî MODEL CANDIDATE SELECTION", automl_plan.get("model_selection_md", "") or f"Rule-based fallback model selection (LLM unavailable)."),
            ("STEP 4 ‚Äî TRAINING & VALIDATION", automl_plan.get("training_validation_md", "") or "Use cross-validation by default with task-appropriate metrics."),
            ("STEP 5 ‚Äî ERROR & BEHAVIOR ANALYSIS", automl_plan.get("error_behavior_analysis_md", "") or "Analyze residuals/confusion matrix and error slices."),
            ("STEP 6 ‚Äî EXPLAINABILITY", automl_plan.get("explainability_md", "") or "Use feature_importances_ when available and align post-encoding names."),
        ]
        # Add planning source note with confidence warning
        plan_quality = automl_plan.get("plan_quality", "high_confidence")
        if planning_source and planning_source != "unknown":
            warning = ""
            if plan_quality == "fallback_low_confidence":
                warning = "‚ö†Ô∏è **LOW-CONFIDENCE FALLBACK PLAN**\n\nThis plan was generated using rule-based fallbacks because the LLM was unavailable or returned invalid responses. **Results may be suboptimal.**\n\n"
            elif plan_quality == "medium_confidence":
                warning = "‚ö†Ô∏è **MEDIUM-CONFIDENCE PLAN**\n\nSome decisions have low confidence scores. Review carefully.\n\n"
            
            planning_error = automl_plan.get("planning_error", "")
            error_note = f"\n**Error:** {planning_error}" if planning_error else ""
            
            md_sections.insert(0, ("PLANNING SOURCE", 
                f"{warning}**Planning Method:** {planning_source.upper()}{error_note}\n\n"
                f"**Target Confidence:** {automl_plan.get('target_confidence', 1.0):.2f}\n"
                f"**Task Confidence:** {automl_plan.get('task_confidence', 1.0):.2f}\n"
                f"**Plan Quality:** {plan_quality.replace('_', ' ').title()}\n"))

    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    f"# ML Model Training: Predicting {target}\n",
                    f"\n",
                    f"This notebook was auto-generated by Intent2Model.\n",
                    f"\n",
                    f"**Task:** {task}\n",
                    f"**Target Column:** {target}\n",
                    f"**Model:** {config.get('model', 'unknown')}\n"
                ]
            },
            *([
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [f"## {title}\n\n{body}\n"]
                }
                for title, body in md_sections
                if body and str(body).strip()
            ]),
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 1. Import Libraries"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "from sklearn.model_selection import train_test_split, cross_val_score\n",
                    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                    "from sklearn.impute import SimpleImputer\n",
                    "from sklearn.pipeline import Pipeline\n",
                    "from sklearn.compose import ColumnTransformer\n",
                    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
                    "import pickle\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "\n",
                    "# Set style\n",
                    "sns.set_style('whitegrid')\n",
                    "plt.rcParams['figure.figsize'] = (12, 6)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 2. Load Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load your dataset\n",
                    "# df = pd.read_csv('your_dataset.csv')\n",
                    "\n",
                    "# For this example, we'll use the uploaded data\n",
                    f"print(f'Dataset shape: {{df.shape}}')\n",
                    f"print(f'Columns: {{list(df.columns)}}')\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 3. Prepare Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        f"# Separate features and target\n",
                        f"X = df.drop(columns=['{target}'])\n",
                        f"y = df['{target}']\n",
                        "\n",
                        "# Handle categorical target if needed\n",
                    ] + 
                    (["le = LabelEncoder()\n", "y = le.fit_transform(y.astype(str))\n"] if task == 'classification' else []) +
                    [
                        "\n",
                        "# Split data\n",
                        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                        "\n",
                        "print(f'Training set: {X_train.shape}')\n",
                        "print(f'Test set: {X_test.shape}')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 4. Build Pipeline"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Identify numeric and categorical columns\n",
                    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                    "\n",
                    "# Build preprocessing dynamically from AutoML plan (if provided)\n",
                    "feature_transforms = " + json.dumps((config or {}).get("feature_transforms", [])) + "\n",
                    "transformers = []\n",
                    "dropped = set([ft['name'] for ft in feature_transforms if ft.get('drop')])\n",
                    "num_scaled = [ft['name'] for ft in feature_transforms if ft.get('name') in numeric_cols and ft.get('scale') == 'standard' and not ft.get('drop')]\n",
                    "num_plain = [c for c in numeric_cols if c not in dropped and c not in num_scaled]\n",
                    "cat_onehot = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'one_hot' and not ft.get('drop')]\n",
                    "cat_ordinal = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'ordinal' and not ft.get('drop')]\n",
                    "cat_freq = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'frequency' and not ft.get('drop')]\n",
                    "\n",
                    "if num_scaled:\n",
                    "    transformers.append(('num_scaled', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_scaled))\n",
                    "if num_plain:\n",
                    "    transformers.append(('num', Pipeline([('imputer', SimpleImputer(strategy='median'))]), num_plain))\n",
                    "if cat_onehot:\n",
                    "    try:\n",
                    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=5)\n",
                    "    except TypeError:\n",
                    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
                    "    transformers.append(('cat_onehot', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', ohe)]), cat_onehot))\n",
                    "if cat_ordinal:\n",
                    "    from sklearn.preprocessing import OrdinalEncoder\n",
                    "    transformers.append(('cat_ordinal', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))]), cat_ordinal))\n",
                    "if cat_freq:\n",
                    "    # Simple frequency encoding\n",
                    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
                    "    class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
                    "        def fit(self, X, y=None):\n",
                    "            import numpy as np\n",
                    "            X = np.asarray(X, dtype=object)\n",
                    "            self.maps_ = []\n",
                    "            for j in range(X.shape[1]):\n",
                    "                col = [\"\" if v is None else str(v) for v in X[:, j]]\n",
                    "                counts = {}\n",
                    "                for v in col:\n",
                    "                    counts[v] = counts.get(v, 0) + 1\n",
                    "                n = float(max(1, len(col)))\n",
                    "                self.maps_.append({k: c / n for k, c in counts.items()})\n",
                    "            return self\n",
                    "        def transform(self, X):\n",
                    "            import numpy as np\n",
                    "            X = np.asarray(X, dtype=object)\n",
                    "            out = np.zeros((X.shape[0], X.shape[1]), dtype=float)\n",
                    "            for j in range(X.shape[1]):\n",
                    "                m = self.maps_[j]\n",
                    "                col = [\"\" if v is None else str(v) for v in X[:, j]]\n",
                    "                out[:, j] = [float(m.get(v, 0.0)) for v in col]\n",
                    "            return out\n",
                    "    transformers.append(('cat_freq', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('freq', FrequencyEncoder())]), cat_freq))\n",
                    "\n",
                    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
                    "\n",
                    "# Create model (auto-generated for this dataset/run)\n",
                ] + (
                    [f"model = {config.get('model_code')}\n"]
                    if config and config.get("model_code")
                    else (
                        ["model = RandomForestClassifier(n_estimators=200, random_state=42)\n"]
                        if task == 'classification'
                        else ["model = RandomForestRegressor(n_estimators=200, random_state=42)\n"]
                    )
                ) + [
                    "\n",
                    "pipeline = Pipeline(steps=[\n",
                    "    ('preprocessor', preprocessor),\n",
                    "    ('model', model)\n",
                    "])"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 5. Train Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Train the model\n",
                        "pipeline.fit(X_train, y_train)\n",
                        "\n",
                        "# Make predictions\n",
                        "y_pred = pipeline.predict(X_test)\n",
                        "\n",
                        "# Evaluate\n",
                    ] + (
                        ["score = accuracy_score(y_test, y_pred)\n", "print(f'Accuracy: {score:.4f}')\n", "\n", "print(classification_report(y_test, y_pred))"] 
                        if task == 'classification' 
                        else ["score = r2_score(y_test, y_pred)\n", "print(f'R2 Score: {score:.4f}')\n", "\n", "print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')"]
                    )
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 6. Feature Importance"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Get feature importance\n",
                    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
                    "    importances = pipeline.named_steps['model'].feature_importances_\n",
                    "    feature_names = numeric_cols + categorical_cols\n",
                    "    \n",
                    "    # Create importance DataFrame\n",
                    "    importance_df = pd.DataFrame({\n",
                    "        'feature': feature_names[:len(importances)],\n",
                    "        'importance': importances\n",
                    "    }).sort_values('importance', ascending=False)\n",
                    "    \n",
                    "    # Plot\n",
                    "    plt.figure(figsize=(10, 6))\n",
                    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
                    "    plt.title('Top 10 Feature Importance')\n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "    \n",
                    "    print(importance_df)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 7. Save Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Save the trained model\n",
                        "with open('model.pkl', 'wb') as f:\n",
                        "    pickle.dump(pipeline, f)\n",
                    ] + (
                        ["\n", "# Save label encoder if used\n", "with open('label_encoder.pkl', 'wb') as f:\n", "    pickle.dump(le, f)\n"]
                        if task == 'classification'
                        else []
                    ) + [
                        "\n",
                        "print('Model saved successfully!')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 8. Make Predictions"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load model for predictions\n",
                    "# with open('model.pkl', 'rb') as f:\n",
                    "#     loaded_model = pickle.load(f)\n",
                    "\n",
                    "# Example prediction\n",
                    "# new_data = pd.DataFrame({...})\n",
                    "# prediction = loaded_model.predict(new_data)\n",
                    "# print(f'Prediction: {prediction}')"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.10.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    return json.dumps(notebook, indent=1)


def generate_readme(
    target: str,
    task: str,
    metrics: Dict[str, float],
    config: Dict[str, Any],
    feature_importance: Optional[Dict[str, float]] = None,
    dataset_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    Generate a comprehensive README.md file.
    """
    readme = f"""# ML Model: Predicting {target}

This model was auto-generated by **Intent2Model** - an LLM-guided AutoML platform.

## üìä Model Overview

- **Task Type:** {task.title()}
- **Target Column:** `{target}`
- **Model Architecture:** {config.get('model', 'Unknown')}
- **Preprocessing:** {', '.join(config.get('preprocessing', [])) or 'None'}

## üìà Performance Metrics

"""
    
    for metric_name, metric_value in metrics.items():
        readme += f"- **{metric_name.upper()}:** {metric_value:.4f}\n"
    
    readme += f"""
## üîß Model Details

### Preprocessing Steps
"""
    
    preprocessing = config.get('preprocessing', [])
    if preprocessing:
        for step in preprocessing:
            readme += f"- {step.replace('_', ' ').title()}\n"
    else:
        readme += "- No preprocessing applied\n"
    
    readme += f"""
### Model Configuration
- **Algorithm:** {config.get('model', 'Unknown')}
- **Task:** {task}

## üì¶ Files Included

- `model.pkl` - Trained model (pickle format)
- `training_notebook.ipynb` - Complete Jupyter notebook with training code
- `README.md` - This file
- `charts/` - Visualization charts (if generated)

## üöÄ Usage

### Load and Use the Model

```python
import pickle
import pandas as pd

# Load the model
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

# Prepare your data (same format as training data)
new_data = pd.DataFrame({{
    # Your feature columns here
}})

# Make prediction
prediction = model.predict(new_data)
print(f'Prediction: {{prediction}}')
```

## üìä Feature Importance

"""
    
    if feature_importance:
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        readme += "Top features by importance:\n\n"
        for i, (feature, importance) in enumerate(sorted_features[:10], 1):
            readme += f"{i}. **{feature}**: {importance:.4f}\n"
    else:
        readme += "Feature importance not available.\n"
    
    readme += f"""
## üîÑ Retraining

To retrain the model, use the provided Jupyter notebook (`training_notebook.ipynb`).

## üìù Notes

- Model was trained using cross-validation
- All preprocessing steps are included in the pipeline
- The model can be used directly for predictions

## ü§ñ Generated by Intent2Model

This model was automatically generated by Intent2Model's LLM-guided AutoML system.
For more information, visit: https://github.com/lakshitsachdeva/intent2model

---
*Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return readme


def save_model_pickle(model: Any, filepath: str) -> str:
    """
    Save model as pickle file and return base64 encoded string.
    """
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    
    with open(filepath, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')


def generate_chart_image(
    chart_type: str,
    data: Dict[str, Any],
    title: str = ""
) -> str:
    """
    Generate chart as base64 encoded image.
    """
    plt.figure(figsize=(10, 6))
    
    if chart_type == "metrics":
        metrics_data = data.get("data", [])
        names = [d["name"] for d in metrics_data]
        values = [d["value"] for d in metrics_data]
        
        plt.bar(names, values, color='steelblue')
        plt.title(title or "Performance Metrics")
        plt.ylabel("Score")
        plt.xticks(rotation=45, ha='right')
        
    elif chart_type == "feature_importance":
        features = list(data.keys())[:10]
        importances = [data[f] for f in features]
        
        plt.barh(features, importances, color='coral')
        plt.title(title or "Feature Importance")
        plt.xlabel("Importance")
        
    elif chart_type == "cv_scores":
        scores = data.get("scores", [])
        folds = [f"Fold {i+1}" for i in range(len(scores))]
        
        plt.plot(folds, scores, marker='o', linewidth=2, markersize=8)
        plt.title(title or "Cross-Validation Scores")
        plt.ylabel("Score")
        plt.xlabel("Fold")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Convert to base64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    image_base64 = base64.b64encode(buffer.read()).decode('utf-8')
    plt.close()
    
    return image_base64


def generate_model_report(
    all_models: list,
    target: str,
    task: str,
    dataset_info: Dict[str, Any],
    trace: list = None,
    preprocessing_recommendations: list = None
) -> str:
    """
    Generate a detailed markdown report with all model explanations, comparisons, and recommendations.
    """
    report_lines = []
    
    # Header
    report_lines.append(f"# Model Training Report: Predicting {target}\n")
    report_lines.append(f"**Generated by:** Intent2Model AutoML Platform\n")
    report_lines.append(f"**Task Type:** {task.capitalize()}\n")
    report_lines.append(f"**Target Column:** {target}\n")
    report_lines.append(f"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report_lines.append("\n---\n")
    
    # Dataset Info
    if dataset_info:
        report_lines.append("## Dataset Overview\n")
        report_lines.append(f"- **Rows:** {dataset_info.get('n_rows', 'N/A')}")
        report_lines.append(f"- **Columns:** {dataset_info.get('n_cols', 'N/A')}")
        report_lines.append(f"- **Numeric Columns:** {len(dataset_info.get('numeric_cols', []))}")
        report_lines.append(f"- **Categorical Columns:** {len(dataset_info.get('categorical_cols', []))}")
        report_lines.append("\n---\n")
    
    # Preprocessing Recommendations
    if preprocessing_recommendations:
        report_lines.append("## Preprocessing Recommendations\n")
        for rec in preprocessing_recommendations:
            report_lines.append(f"### {rec.get('type', 'General').capitalize()}")
            report_lines.append(f"**Why:** {rec.get('why', 'N/A')}")
            report_lines.append(f"**Suggestion:** {rec.get('suggestion', 'N/A')}")
            report_lines.append("")
        report_lines.append("---\n")
    
    # Training Trace
    if trace:
        report_lines.append("## Training Process\n")
        for i, step in enumerate(trace, 1):
            report_lines.append(f"{i}. {step}")
        report_lines.append("\n---\n")
    
    # Model Comparison
    report_lines.append("## Model Comparison\n")
    report_lines.append("| Model | Primary Metric | CV Mean | CV Std | Status |")
    report_lines.append("|-------|---------------|---------|--------|--------|")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        primary_metric = model.get('primary_metric')
        if primary_metric is None:
            # Try to get from metrics dict
            metrics_dict = model.get('metrics', {})
            if metrics_dict:
                # Use first metric value as fallback
                primary_metric = next(iter(metrics_dict.values()), 0)
            else:
                primary_metric = 0
        primary_metric = float(primary_metric) if primary_metric is not None else 0.0
        cv_mean = float(model.get('cv_mean', 0)) if model.get('cv_mean') is not None else 0.0
        cv_std = float(model.get('cv_std', 0)) if model.get('cv_std') is not None else 0.0
        status = "‚≠ê Best" if idx == 0 else "Available"
        
        report_lines.append(f"| {model_name} | {primary_metric:.4f} | {cv_mean:.4f} | {cv_std:.4f} | {status} |")
    
    report_lines.append("\n---\n")
    
    # Detailed Model Explanations
    report_lines.append("## Detailed Model Analysis\n")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        is_best = idx == 0
        
        report_lines.append(f"### {model_name} {'‚≠ê (Recommended)' if is_best else ''}\n")
        
        # Metrics
        metrics_dict = model.get('metrics', {})
        if metrics_dict:
            report_lines.append("#### Performance Metrics\n")
            for metric_name, value in metrics_dict.items():
                if value is not None:
                    try:
                        report_lines.append(f"- **{metric_name}:** {float(value):.4f}")
                    except (ValueError, TypeError):
                        report_lines.append(f"- **{metric_name}:** {value}")
            report_lines.append("")
        
        # Primary metric highlight
        if model.get('primary_metric') is not None:
            report_lines.append(f"**Primary Metric Score:** {float(model.get('primary_metric', 0)):.4f}\n")
        
        # Full Explanation (if available from LLM explainer)
        explanation = model.get('explanation', {})
        if explanation:
            if isinstance(explanation, dict):
                if explanation.get('explanation'):
                    report_lines.append("#### Analysis\n")
                    report_lines.append(explanation.get('explanation'))
                    report_lines.append("")
                
                if explanation.get('strengths'):
                    report_lines.append("#### Strengths\n")
                    report_lines.append(explanation.get('strengths'))
                    report_lines.append("")
                
                if explanation.get('recommendation'):
                    report_lines.append("#### Recommendations\n")
                    report_lines.append(explanation.get('recommendation'))
                    report_lines.append("")
            elif isinstance(explanation, str):
                report_lines.append("#### Analysis\n")
                report_lines.append(explanation)
                report_lines.append("")
        
        # CV Scores
        if model.get('cv_scores'):
            report_lines.append("#### Cross-Validation Scores\n")
            cv_scores = model.get('cv_scores', [])
            report_lines.append(f"Individual fold scores: {', '.join([f'{s:.4f}' for s in cv_scores])}")
            report_lines.append(f"Mean: {model.get('cv_mean', 0):.4f} ¬± {model.get('cv_std', 0):.4f}")
            report_lines.append("")
        
        # Feature Importance (if available)
        if model.get('feature_importance'):
            report_lines.append("#### Top Feature Importance\n")
            feat_imp = model.get('feature_importance', {})
            sorted_features = sorted(feat_imp.items(), key=lambda x: x[1], reverse=True)[:10]
            for feat, imp in sorted_features:
                report_lines.append(f"- **{feat}:** {imp:.4f}")
            report_lines.append("")
        
        report_lines.append("---\n")
    
    # Conclusion
    report_lines.append("## Conclusion\n")
    if all_models:
        best_model = all_models[0]
        best_name = best_model.get('model_name', 'Unknown').replace('_', ' ').title()
        report_lines.append(f"The **{best_name}** model performed best for this dataset and task.")
        report_lines.append("Consider the detailed analysis above when making deployment decisions.")
        report_lines.append("\nFor questions or further analysis, refer to the generated Jupyter notebook.")
    
    return "\n".join(report_lines)
