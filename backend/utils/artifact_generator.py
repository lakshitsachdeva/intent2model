"""
Generate artifacts: Jupyter notebook, pickle file, charts, README
"""

import json
import pickle
import base64
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import numpy as np


def generate_notebook(
    df: pd.DataFrame,
    target: str,
    task: str,
    config: Dict[str, Any],
    metrics: Dict[str, float],
    feature_importance: Optional[Dict[str, float]] = None,
    model: Any = None
) -> str:
    """
    Generate a Jupyter notebook with complete training code.
    """
    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    f"# ML Model Training: Predicting {target}\n",
                    f"\n",
                    f"This notebook was auto-generated by Intent2Model.\n",
                    f"\n",
                    f"**Task:** {task}\n",
                    f"**Target Column:** {target}\n",
                    f"**Model:** {config.get('model', 'unknown')}\n"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 1. Import Libraries"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "from sklearn.model_selection import train_test_split, cross_val_score\n",
                    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                    "from sklearn.impute import SimpleImputer\n",
                    "from sklearn.pipeline import Pipeline\n",
                    "from sklearn.compose import ColumnTransformer\n",
                    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
                    "import pickle\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "\n",
                    "# Set style\n",
                    "sns.set_style('whitegrid')\n",
                    "plt.rcParams['figure.figsize'] = (12, 6)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 2. Load Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load your dataset\n",
                    "# df = pd.read_csv('your_dataset.csv')\n",
                    "\n",
                    "# For this example, we'll use the uploaded data\n",
                    f"print(f'Dataset shape: {{df.shape}}')\n",
                    f"print(f'Columns: {{list(df.columns)}}')\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 3. Prepare Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        f"# Separate features and target\n",
                        f"X = df.drop(columns=['{target}'])\n",
                        f"y = df['{target}']\n",
                        "\n",
                        "# Handle categorical target if needed\n",
                    ] + 
                    (["le = LabelEncoder()\n", "y = le.fit_transform(y.astype(str))\n"] if task == 'classification' else []) +
                    [
                        "\n",
                        "# Split data\n",
                        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                        "\n",
                        "print(f'Training set: {X_train.shape}')\n",
                        "print(f'Test set: {X_test.shape}')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 4. Build Pipeline"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Identify numeric and categorical columns\n",
                    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                    "\n",
                    "# Create preprocessing steps\n",
                    "numeric_transformer = Pipeline(steps=[\n",
                    "    ('imputer', SimpleImputer(strategy='mean')),\n",
                    "    ('scaler', StandardScaler())\n",
                    "])\n",
                    "\n",
                    "categorical_transformer = Pipeline(steps=[\n",
                    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                    "])\n",
                    "\n",
                    "# Combine preprocessing\n",
                    "preprocessor = ColumnTransformer(\n",
                    "    transformers=[\n",
                    "        ('num', numeric_transformer, numeric_cols),\n",
                    "        ('cat', categorical_transformer, categorical_cols)\n",
                    "    ]\n",
                    ")\n",
                    "\n",
                    "# Create model (auto-generated for this dataset/run)\n",
                ] + (
                    [f"model = {config.get('model_code')}\n"]
                    if config and config.get("model_code")
                    else (
                        ["model = RandomForestClassifier(n_estimators=200, random_state=42)\n"]
                        if task == 'classification'
                        else ["model = RandomForestRegressor(n_estimators=200, random_state=42)\n"]
                    )
                ) + [
                    "\n",
                    "pipeline = Pipeline(steps=[\n",
                    "    ('preprocessor', preprocessor),\n",
                    "    ('model', model)\n",
                    "])"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 5. Train Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Train the model\n",
                        "pipeline.fit(X_train, y_train)\n",
                        "\n",
                        "# Make predictions\n",
                        "y_pred = pipeline.predict(X_test)\n",
                        "\n",
                        "# Evaluate\n",
                    ] + (
                        ["score = accuracy_score(y_test, y_pred)\n", "print(f'Accuracy: {score:.4f}')\n", "\n", "print(classification_report(y_test, y_pred))"] 
                        if task == 'classification' 
                        else ["score = r2_score(y_test, y_pred)\n", "print(f'R2 Score: {score:.4f}')\n", "\n", "print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')"]
                    )
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 6. Feature Importance"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Get feature importance\n",
                    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
                    "    importances = pipeline.named_steps['model'].feature_importances_\n",
                    "    feature_names = numeric_cols + categorical_cols\n",
                    "    \n",
                    "    # Create importance DataFrame\n",
                    "    importance_df = pd.DataFrame({\n",
                    "        'feature': feature_names[:len(importances)],\n",
                    "        'importance': importances\n",
                    "    }).sort_values('importance', ascending=False)\n",
                    "    \n",
                    "    # Plot\n",
                    "    plt.figure(figsize=(10, 6))\n",
                    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
                    "    plt.title('Top 10 Feature Importance')\n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "    \n",
                    "    print(importance_df)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 7. Save Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Save the trained model\n",
                        "with open('model.pkl', 'wb') as f:\n",
                        "    pickle.dump(pipeline, f)\n",
                    ] + (
                        ["\n", "# Save label encoder if used\n", "with open('label_encoder.pkl', 'wb') as f:\n", "    pickle.dump(le, f)\n"]
                        if task == 'classification'
                        else []
                    ) + [
                        "\n",
                        "print('Model saved successfully!')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 8. Make Predictions"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load model for predictions\n",
                    "# with open('model.pkl', 'rb') as f:\n",
                    "#     loaded_model = pickle.load(f)\n",
                    "\n",
                    "# Example prediction\n",
                    "# new_data = pd.DataFrame({...})\n",
                    "# prediction = loaded_model.predict(new_data)\n",
                    "# print(f'Prediction: {prediction}')"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.10.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    return json.dumps(notebook, indent=1)


def generate_readme(
    target: str,
    task: str,
    metrics: Dict[str, float],
    config: Dict[str, Any],
    feature_importance: Optional[Dict[str, float]] = None,
    dataset_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    Generate a comprehensive README.md file.
    """
    readme = f"""# ML Model: Predicting {target}

This model was auto-generated by **Intent2Model** - an LLM-guided AutoML platform.

## üìä Model Overview

- **Task Type:** {task.title()}
- **Target Column:** `{target}`
- **Model Architecture:** {config.get('model', 'Unknown')}
- **Preprocessing:** {', '.join(config.get('preprocessing', [])) or 'None'}

## üìà Performance Metrics

"""
    
    for metric_name, metric_value in metrics.items():
        readme += f"- **{metric_name.upper()}:** {metric_value:.4f}\n"
    
    readme += f"""
## üîß Model Details

### Preprocessing Steps
"""
    
    preprocessing = config.get('preprocessing', [])
    if preprocessing:
        for step in preprocessing:
            readme += f"- {step.replace('_', ' ').title()}\n"
    else:
        readme += "- No preprocessing applied\n"
    
    readme += f"""
### Model Configuration
- **Algorithm:** {config.get('model', 'Unknown')}
- **Task:** {task}

## üì¶ Files Included

- `model.pkl` - Trained model (pickle format)
- `training_notebook.ipynb` - Complete Jupyter notebook with training code
- `README.md` - This file
- `charts/` - Visualization charts (if generated)

## üöÄ Usage

### Load and Use the Model

```python
import pickle
import pandas as pd

# Load the model
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

# Prepare your data (same format as training data)
new_data = pd.DataFrame({{
    # Your feature columns here
}})

# Make prediction
prediction = model.predict(new_data)
print(f'Prediction: {{prediction}}')
```

## üìä Feature Importance

"""
    
    if feature_importance:
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        readme += "Top features by importance:\n\n"
        for i, (feature, importance) in enumerate(sorted_features[:10], 1):
            readme += f"{i}. **{feature}**: {importance:.4f}\n"
    else:
        readme += "Feature importance not available.\n"
    
    readme += f"""
## üîÑ Retraining

To retrain the model, use the provided Jupyter notebook (`training_notebook.ipynb`).

## üìù Notes

- Model was trained using cross-validation
- All preprocessing steps are included in the pipeline
- The model can be used directly for predictions

## ü§ñ Generated by Intent2Model

This model was automatically generated by Intent2Model's LLM-guided AutoML system.
For more information, visit: https://github.com/lakshitsachdeva/intent2model

---
*Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return readme


def save_model_pickle(model: Any, filepath: str) -> str:
    """
    Save model as pickle file and return base64 encoded string.
    """
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    
    with open(filepath, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')


def generate_chart_image(
    chart_type: str,
    data: Dict[str, Any],
    title: str = ""
) -> str:
    """
    Generate chart as base64 encoded image.
    """
    plt.figure(figsize=(10, 6))
    
    if chart_type == "metrics":
        metrics_data = data.get("data", [])
        names = [d["name"] for d in metrics_data]
        values = [d["value"] for d in metrics_data]
        
        plt.bar(names, values, color='steelblue')
        plt.title(title or "Performance Metrics")
        plt.ylabel("Score")
        plt.xticks(rotation=45, ha='right')
        
    elif chart_type == "feature_importance":
        features = list(data.keys())[:10]
        importances = [data[f] for f in features]
        
        plt.barh(features, importances, color='coral')
        plt.title(title or "Feature Importance")
        plt.xlabel("Importance")
        
    elif chart_type == "cv_scores":
        scores = data.get("scores", [])
        folds = [f"Fold {i+1}" for i in range(len(scores))]
        
        plt.plot(folds, scores, marker='o', linewidth=2, markersize=8)
        plt.title(title or "Cross-Validation Scores")
        plt.ylabel("Score")
        plt.xlabel("Fold")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Convert to base64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    image_base64 = base64.b64encode(buffer.read()).decode('utf-8')
    plt.close()
    
    return image_base64


def generate_model_report(
    all_models: list,
    target: str,
    task: str,
    dataset_info: Dict[str, Any],
    trace: list = None,
    preprocessing_recommendations: list = None
) -> str:
    """
    Generate a detailed markdown report with all model explanations, comparisons, and recommendations.
    """
    report_lines = []
    
    # Header
    report_lines.append(f"# Model Training Report: Predicting {target}\n")
    report_lines.append(f"**Generated by:** Intent2Model AutoML Platform\n")
    report_lines.append(f"**Task Type:** {task.capitalize()}\n")
    report_lines.append(f"**Target Column:** {target}\n")
    report_lines.append(f"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report_lines.append("\n---\n")
    
    # Dataset Info
    if dataset_info:
        report_lines.append("## Dataset Overview\n")
        report_lines.append(f"- **Rows:** {dataset_info.get('n_rows', 'N/A')}")
        report_lines.append(f"- **Columns:** {dataset_info.get('n_cols', 'N/A')}")
        report_lines.append(f"- **Numeric Columns:** {len(dataset_info.get('numeric_cols', []))}")
        report_lines.append(f"- **Categorical Columns:** {len(dataset_info.get('categorical_cols', []))}")
        report_lines.append("\n---\n")
    
    # Preprocessing Recommendations
    if preprocessing_recommendations:
        report_lines.append("## Preprocessing Recommendations\n")
        for rec in preprocessing_recommendations:
            report_lines.append(f"### {rec.get('type', 'General').capitalize()}")
            report_lines.append(f"**Why:** {rec.get('why', 'N/A')}")
            report_lines.append(f"**Suggestion:** {rec.get('suggestion', 'N/A')}")
            report_lines.append("")
        report_lines.append("---\n")
    
    # Training Trace
    if trace:
        report_lines.append("## Training Process\n")
        for i, step in enumerate(trace, 1):
            report_lines.append(f"{i}. {step}")
        report_lines.append("\n---\n")
    
    # Model Comparison
    report_lines.append("## Model Comparison\n")
    report_lines.append("| Model | Primary Metric | CV Mean | CV Std | Status |")
    report_lines.append("|-------|---------------|---------|--------|--------|")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        primary_metric = model.get('primary_metric', 0)
        cv_mean = model.get('cv_mean', 0)
        cv_std = model.get('cv_std', 0)
        status = "‚≠ê Best" if idx == 0 else "Available"
        
        report_lines.append(f"| {model_name} | {primary_metric:.4f} | {cv_mean:.4f} | {cv_std:.4f} | {status} |")
    
    report_lines.append("\n---\n")
    
    # Detailed Model Explanations
    report_lines.append("## Detailed Model Analysis\n")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        is_best = idx == 0
        
        report_lines.append(f"### {model_name} {'‚≠ê (Recommended)' if is_best else ''}\n")
        
        # Metrics
        if model.get('metrics'):
            report_lines.append("#### Performance Metrics\n")
            for metric_name, value in model.get('metrics', {}).items():
                report_lines.append(f"- **{metric_name}:** {value:.4f}")
            report_lines.append("")
        
        # Full Explanation
        explanation = model.get('explanation', {})
        if explanation:
            if isinstance(explanation, dict):
                if explanation.get('explanation'):
                    report_lines.append("#### Analysis\n")
                    report_lines.append(explanation.get('explanation'))
                    report_lines.append("")
                
                if explanation.get('strengths'):
                    report_lines.append("#### Strengths\n")
                    report_lines.append(explanation.get('strengths'))
                    report_lines.append("")
                
                if explanation.get('recommendation'):
                    report_lines.append("#### Recommendations\n")
                    report_lines.append(explanation.get('recommendation'))
                    report_lines.append("")
            elif isinstance(explanation, str):
                report_lines.append("#### Analysis\n")
                report_lines.append(explanation)
                report_lines.append("")
        
        # CV Scores
        if model.get('cv_scores'):
            report_lines.append("#### Cross-Validation Scores\n")
            cv_scores = model.get('cv_scores', [])
            report_lines.append(f"Individual fold scores: {', '.join([f'{s:.4f}' for s in cv_scores])}")
            report_lines.append(f"Mean: {model.get('cv_mean', 0):.4f} ¬± {model.get('cv_std', 0):.4f}")
            report_lines.append("")
        
        # Feature Importance (if available)
        if model.get('feature_importance'):
            report_lines.append("#### Top Feature Importance\n")
            feat_imp = model.get('feature_importance', {})
            sorted_features = sorted(feat_imp.items(), key=lambda x: x[1], reverse=True)[:10]
            for feat, imp in sorted_features:
                report_lines.append(f"- **{feat}:** {imp:.4f}")
            report_lines.append("")
        
        report_lines.append("---\n")
    
    # Conclusion
    report_lines.append("## Conclusion\n")
    if all_models:
        best_model = all_models[0]
        best_name = best_model.get('model_name', 'Unknown').replace('_', ' ').title()
        report_lines.append(f"The **{best_name}** model performed best for this dataset and task.")
        report_lines.append("Consider the detailed analysis above when making deployment decisions.")
        report_lines.append("\nFor questions or further analysis, refer to the generated Jupyter notebook.")
    
    return "\n".join(report_lines)
