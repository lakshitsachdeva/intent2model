"""
Generate artifacts: Jupyter notebook, pickle file, charts, README
"""

import json
import pickle
import base64
from typing import Dict, Any, Optional, List, List
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import numpy as np


def generate_notebook(
    df: pd.DataFrame,
    target: str,
    task: str,
    config: Dict[str, Any],
    metrics: Dict[str, float],
    feature_importance: Optional[Dict[str, float]] = None,
    model: Any = None
) -> str:
    """
    Generate a Jupyter notebook with complete training code.
    
    CRITICAL: Code MUST be generated from AutoMLPlan, not hardcoded boilerplate.
    """
    from schemas.pipeline_schema import AutoMLPlan
    from agents.plan_compiler import (
        compile_preprocessing_code,
        compile_model_code,
        compile_metrics_code,
        compile_pipeline_code,
        validate_plan_for_execution
    )
    
    automl_plan_dict = (config or {}).get("automl_plan") or {}
    
    # Convert dict to AutoMLPlan if needed
    plan = None
    if automl_plan_dict and isinstance(automl_plan_dict, dict):
        try:
            plan = AutoMLPlan(**automl_plan_dict)
            # Validate plan before generating code
            try:
                validate_plan_for_execution(plan)
            except RuntimeError as e:
                # For low-confidence plans, still generate but with warnings
                if "low-confidence" in str(e).lower():
                    print(f"âš ï¸  Warning: {e}")
                else:
                    raise
        except Exception as e:
            print(f"âš ï¸  Could not parse AutoMLPlan: {e}. Using fallback code generation.")
            plan = None
    
    automl_plan = automl_plan_dict  # Keep dict for markdown sections
    # Expect markdown sections in automl_plan if present
    md_sections = []
    if isinstance(automl_plan, dict) and automl_plan:
        # Always show sections if they exist (even if fallback text)
        planning_source = automl_plan.get("planning_source", "unknown")
        md_sections = [
            ("STEP 0 â€” TASK INFERENCE", automl_plan.get("task_inference_md", "") or f"Rule-based fallback task inference (LLM unavailable)."),
            ("STEP 1 â€” DATASET INTELLIGENCE", automl_plan.get("dataset_intelligence_md", "") or f"Rule-based fallback dataset intelligence (LLM unavailable)."),
            ("STEP 2 â€” TRANSFORMATION STRATEGY", automl_plan.get("transformation_strategy_md", "") or f"Rule-based fallback transformation strategy (LLM unavailable)."),
            ("STEP 3 â€” MODEL CANDIDATE SELECTION", automl_plan.get("model_selection_md", "") or f"Rule-based fallback model selection (LLM unavailable)."),
            ("STEP 4 â€” TRAINING & VALIDATION", automl_plan.get("training_validation_md", "") or "Use cross-validation by default with task-appropriate metrics."),
            ("STEP 5 â€” ERROR & BEHAVIOR ANALYSIS", automl_plan.get("error_behavior_analysis_md", "") or "Analyze residuals/confusion matrix and error slices."),
            ("STEP 6 â€” EXPLAINABILITY", automl_plan.get("explainability_md", "") or "Use feature_importances_ when available and align post-encoding names."),
        ]
        # Add planning source note with confidence warning
        plan_quality = automl_plan.get("plan_quality", "high_confidence")
        if planning_source and planning_source != "unknown":
            warning = ""
            if plan_quality == "fallback_low_confidence":
                warning = "âš ï¸ **LOW-CONFIDENCE FALLBACK PLAN**\n\nThis plan was generated using rule-based fallbacks because the LLM was unavailable or returned invalid responses. **Results may be suboptimal.**\n\n"
            elif plan_quality == "medium_confidence":
                warning = "âš ï¸ **MEDIUM-CONFIDENCE PLAN**\n\nSome decisions have low confidence scores. Review carefully.\n\n"
            
            planning_error = automl_plan.get("planning_error", "")
            error_note = f"\n**Error:** {planning_error}" if planning_error else ""
            
            md_sections.insert(0, ("PLANNING SOURCE", 
                f"{warning}**Planning Method:** {planning_source.upper()}{error_note}\n\n"
                f"**Target Confidence:** {automl_plan.get('target_confidence', 1.0):.2f}\n"
                f"**Task Confidence:** {automl_plan.get('task_confidence', 1.0):.2f}\n"
                f"**Plan Quality:** {plan_quality.replace('_', ' ').title()}\n"))

    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    f"# ML Model Training: Predicting {target}\n",
                    f"\n",
                    f"This notebook was auto-generated by Intent2Model.\n",
                    f"\n",
                    f"**Task:** {task}\n",
                    f"**Target Column:** {target}\n",
                    f"**Model:** {config.get('model', 'unknown')}\n"
                ]
            },
            *([
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": [f"## {title}\n\n{body}\n"]
                }
                for title, body in md_sections
                if body and str(body).strip()
            ]),
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 1. Import Libraries"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "from sklearn.model_selection import train_test_split, cross_val_score\n",
                    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                    "from sklearn.impute import SimpleImputer\n",
                    "from sklearn.pipeline import Pipeline\n",
                    "from sklearn.compose import ColumnTransformer\n",
                    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
                    "import pickle\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "\n",
                    "# Set style\n",
                    "sns.set_style('whitegrid')\n",
                    "plt.rcParams['figure.figsize'] = (12, 6)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 2. Load Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load your dataset\n",
                    "# df = pd.read_csv('your_dataset.csv')\n",
                    "\n",
                    "# For this example, we'll use the uploaded data\n",
                    f"print(f'Dataset shape: {{df.shape}}')\n",
                    f"print(f'Columns: {{list(df.columns)}}')\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 3. Prepare Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        f"# Separate features and target\n",
                        f"X = df.drop(columns=['{target}'])\n",
                        f"y = df['{target}']\n",
                        "\n",
                        "# Handle categorical target if needed\n",
                    ] + 
                    (["le = LabelEncoder()\n", "y = le.fit_transform(y.astype(str))\n"] if task == 'classification' else []) +
                    [
                        "\n",
                        "# Split data\n",
                        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                        "\n",
                        "print(f'Training set: {X_train.shape}')\n",
                        "print(f'Test set: {X_test.shape}')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 4. Build Preprocessing Pipeline (from AutoMLPlan)"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [compile_preprocessing_code(plan, df)] if plan else [
                        "# âš ï¸ AutoMLPlan not available - using fallback preprocessing\n",
                        "# This should not happen if LLM planning succeeded\n",
                        "from sklearn.pipeline import Pipeline\n",
                        "from sklearn.compose import ColumnTransformer\n",
                        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                        "from sklearn.impute import SimpleImputer\n",
                        "\n",
                        "# Fallback: basic preprocessing\n",
                        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                        "\n",
                        "transformers = []\n",
                        "if numeric_cols:\n",
                        "    transformers.append(('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols))\n",
                        "if categorical_cols:\n",
                        "    transformers.append(('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_cols))\n",
                        "preprocessor = ColumnTransformer(transformers, remainder='drop')\n"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 5. Build Model (from AutoMLPlan)"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [compile_model_code(plan, config.get('model'))] if plan else [
                        "# âš ï¸ AutoMLPlan not available - using fallback model\n",
                        f"from sklearn.ensemble import {'RandomForestClassifier' if task == 'classification' else 'RandomForestRegressor'}\n",
                        f"model = {'RandomForestClassifier' if task == 'classification' else 'RandomForestRegressor'}(n_estimators=200, random_state=42)\n"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 6. Assemble Pipeline (from AutoMLPlan)"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [compile_pipeline_code(plan)] if plan else [
                        "from sklearn.pipeline import Pipeline\n",
                        "pipeline = Pipeline([\n",
                        "    ('preprocessor', preprocessor),\n",
                        "    ('model', model)\n",
                        "])\n"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 7. Train Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Train the model\n",
                    "pipeline.fit(X_train, y_train)\n",
                    "\n",
                    "# Make predictions\n",
                    "y_pred = pipeline.predict(X_test)\n",
                    "\n",
                    "# Evaluate using metrics from AutoMLPlan\n"
                ] + (
                    [compile_metrics_code(plan)] if plan else [
                        "# âš ï¸ AutoMLPlan not available - using fallback metrics\n",
                        "from sklearn.metrics import accuracy_score, classification_report, r2_score, mean_squared_error\n"
                    ]
                ) + [
                    "\n",
                    "# Calculate metrics\n"
                ] + (
                    _generate_metrics_evaluation_code(plan, task) if plan else (
                        [
                            "score = accuracy_score(y_test, y_pred)\n",
                            "print(f'Accuracy: {score:.4f}')\n",
                            "print(classification_report(y_test, y_pred))\n"
                        ] if task == 'classification' else [
                            "score = r2_score(y_test, y_pred)\n",
                            "print(f'R2 Score: {score:.4f}')\n",
                            "print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\n"
                        ]
                    )
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 8. Feature Importance (from plan.explainability_md)"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Get feature importance (aligned with plan)\n",
                    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
                    "    importances = pipeline.named_steps['model'].feature_importances_\n",
                    "    \n",
                    "    # Get feature names after preprocessing (aligned with plan, not dtype-based)\n",
                    "    # NOTE: Do NOT use numeric_cols or categorical_cols - they may not be defined\n",
                    "    try:\n",
                    "        preprocessor = pipeline.named_steps['preprocessor']\n",
                    "        feature_names = []\n",
                    "        # Get feature names from preprocessor transformers\n",
                    "        if hasattr(preprocessor, 'transformers_'):\n",
                    "            for name, transformer, cols in preprocessor.transformers_:\n",
                    "                if hasattr(transformer, 'get_feature_names_out'):\n",
                    "                    feature_names.extend(transformer.get_feature_names_out(cols))\n",
                    "                elif hasattr(transformer, 'named_steps'):\n",
                    "                    # Pipeline transformer\n",
                    "                    for step_name, step_transformer in transformer.named_steps.items():\n",
                    "                        if hasattr(step_transformer, 'get_feature_names_out'):\n",
                    "                            feature_names.extend(step_transformer.get_feature_names_out(cols))\n",
                    "                            break\n",
                    "                else:\n",
                    "                    # Fallback: use column names\n",
                    "                    feature_names.extend([f'{name}_{col}' for col in cols])\n",
                    "        else:\n",
                    "            # Preprocessor not fitted yet - use generic names\n",
                    "            feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
                    "    except Exception as e:\n",
                    "        # Fallback: use generic feature names\n",
                    "        feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
                    "    \n",
                    "    # Create importance DataFrame\n",
                    "    importance_df = pd.DataFrame({\n",
                    "        'feature': feature_names[:len(importances)],\n",
                    "        'importance': importances\n",
                    "    }).sort_values('importance', ascending=False)\n",
                    "    \n",
                    "    # Plot\n",
                    "    plt.figure(figsize=(10, 6))\n",
                    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
                    "    plt.title('Top 10 Feature Importance')\n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "    \n",
                    "    print(importance_df)\n",
                    "else:\n",
                    "    print('Feature importance not available for this model type.')"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 9. Save Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Save the trained model\n",
                        "with open('model.pkl', 'wb') as f:\n",
                        "    pickle.dump(pipeline, f)\n",
                    ] + (
                        ["\n", "# Save label encoder if used\n", "with open('label_encoder.pkl', 'wb') as f:\n", "    pickle.dump(le, f)\n"]
                        if task == 'classification'
                        else []
                    ) + [
                        "\n",
                        "print('Model saved successfully!')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 10. Make Predictions"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load model for predictions\n",
                    "# with open('model.pkl', 'rb') as f:\n",
                    "#     loaded_model = pickle.load(f)\n",
                    "\n",
                    "# Example prediction\n",
                    "# new_data = pd.DataFrame({...})\n",
                    "# prediction = loaded_model.predict(new_data)\n",
                    "# print(f'Prediction: {prediction}')"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.10.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    return json.dumps(notebook, indent=1)


def generate_readme(
    target: str,
    task: str,
    metrics: Dict[str, float],
    config: Dict[str, Any],
    feature_importance: Optional[Dict[str, float]] = None,
    dataset_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    Generate a comprehensive README.md file.
    """
    readme = f"""# ML Model: Predicting {target}

This model was auto-generated by **Intent2Model** - an LLM-guided AutoML platform.

## ðŸ“Š Model Overview

- **Task Type:** {task.title()}
- **Target Column:** `{target}`
- **Model Architecture:** {config.get('model', 'Unknown')}
- **Preprocessing:** {', '.join(config.get('preprocessing', [])) or 'None'}

## ðŸ“ˆ Performance Metrics

"""
    
    for metric_name, metric_value in metrics.items():
        readme += f"- **{metric_name.upper()}:** {metric_value:.4f}\n"
    
    readme += f"""
## ðŸ”§ Model Details

### Preprocessing Steps
"""
    
    preprocessing = config.get('preprocessing', [])
    if preprocessing:
        for step in preprocessing:
            readme += f"- {step.replace('_', ' ').title()}\n"
    else:
        readme += "- No preprocessing applied\n"
    
    readme += f"""
### Model Configuration
- **Algorithm:** {config.get('model', 'Unknown')}
- **Task:** {task}

## ðŸ“¦ Files Included

- `model.pkl` - Trained model (pickle format)
- `training_notebook.ipynb` - Complete Jupyter notebook with training code
- `README.md` - This file
- `charts/` - Visualization charts (if generated)

## ðŸš€ Usage

### Load and Use the Model

```python
import pickle
import pandas as pd

# Load the model
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

# Prepare your data (same format as training data)
new_data = pd.DataFrame({{
    # Your feature columns here
}})

# Make prediction
prediction = model.predict(new_data)
print(f'Prediction: {{prediction}}')
```

## ðŸ“Š Feature Importance

"""
    
    if feature_importance:
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        readme += "Top features by importance:\n\n"
        for i, (feature, importance) in enumerate(sorted_features[:10], 1):
            readme += f"{i}. **{feature}**: {importance:.4f}\n"
    else:
        readme += "Feature importance not available.\n"
    
    readme += f"""
## ðŸ”„ Retraining

To retrain the model, use the provided Jupyter notebook (`training_notebook.ipynb`).

## ðŸ“ Notes

- Model was trained using cross-validation
- All preprocessing steps are included in the pipeline
- The model can be used directly for predictions

## ðŸ¤– Generated by Intent2Model

This model was automatically generated by Intent2Model's LLM-guided AutoML system.
For more information, visit: https://github.com/lakshitsachdeva/intent2model

---
*Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return readme


def save_model_pickle(model: Any, filepath: str) -> str:
    """
    Save model as pickle file and return base64 encoded string.
    """
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    
    with open(filepath, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')


def generate_chart_image(
    chart_type: str,
    data: Dict[str, Any],
    title: str = ""
) -> str:
    """
    Generate chart as base64 encoded image.
    """
    plt.figure(figsize=(10, 6))
    
    if chart_type == "metrics":
        metrics_data = data.get("data", [])
        names = [d["name"] for d in metrics_data]
        values = [d["value"] for d in metrics_data]
        
        plt.bar(names, values, color='steelblue')
        plt.title(title or "Performance Metrics")
        plt.ylabel("Score")
        plt.xticks(rotation=45, ha='right')
        
    elif chart_type == "feature_importance":
        features = list(data.keys())[:10]
        importances = [data[f] for f in features]
        
        plt.barh(features, importances, color='coral')
        plt.title(title or "Feature Importance")
        plt.xlabel("Importance")
        
    elif chart_type == "cv_scores":
        scores = data.get("scores", [])
        folds = [f"Fold {i+1}" for i in range(len(scores))]
        
        plt.plot(folds, scores, marker='o', linewidth=2, markersize=8)
        plt.title(title or "Cross-Validation Scores")
        plt.ylabel("Score")
        plt.xlabel("Fold")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Convert to base64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    image_base64 = base64.b64encode(buffer.read()).decode('utf-8')
    plt.close()
    
    return image_base64


def generate_model_report(
    all_models: list,
    target: str,
    task: str,
    dataset_info: Dict[str, Any],
    trace: list = None,
    preprocessing_recommendations: list = None
) -> str:
    """
    Generate a detailed markdown report with all model explanations, comparisons, and recommendations.
    """
    report_lines = []
    
    # Header
    report_lines.append(f"# Model Training Report: Predicting {target}\n")
    report_lines.append(f"**Generated by:** Intent2Model AutoML Platform\n")
    report_lines.append(f"**Task Type:** {task.capitalize()}\n")
    report_lines.append(f"**Target Column:** {target}\n")
    report_lines.append(f"**Date:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report_lines.append("\n---\n")
    
    # Dataset Info
    if dataset_info:
        report_lines.append("## Dataset Overview\n")
        report_lines.append(f"- **Rows:** {dataset_info.get('n_rows', 'N/A')}")
        report_lines.append(f"- **Columns:** {dataset_info.get('n_cols', 'N/A')}")
        report_lines.append(f"- **Numeric Columns:** {len(dataset_info.get('numeric_cols', []))}")
        report_lines.append(f"- **Categorical Columns:** {len(dataset_info.get('categorical_cols', []))}")
        report_lines.append("\n---\n")
    
    # Preprocessing Recommendations
    if preprocessing_recommendations:
        report_lines.append("## Preprocessing Recommendations\n")
        for rec in preprocessing_recommendations:
            report_lines.append(f"### {rec.get('type', 'General').capitalize()}")
            report_lines.append(f"**Why:** {rec.get('why', 'N/A')}")
            report_lines.append(f"**Suggestion:** {rec.get('suggestion', 'N/A')}")
            report_lines.append("")
        report_lines.append("---\n")
    
    # Training Trace
    if trace:
        report_lines.append("## Training Process\n")
        for i, step in enumerate(trace, 1):
            report_lines.append(f"{i}. {step}")
        report_lines.append("\n---\n")
    
    # Model Comparison
    report_lines.append("## Model Comparison\n")
    report_lines.append("| Model | Primary Metric | CV Mean | CV Std | Status |")
    report_lines.append("|-------|---------------|---------|--------|--------|")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        primary_metric = model.get('primary_metric')
        if primary_metric is None:
            # Try to get from metrics dict
            metrics_dict = model.get('metrics', {})
            if metrics_dict:
                # Use first metric value as fallback
                primary_metric = next(iter(metrics_dict.values()), 0)
            else:
                primary_metric = 0
        primary_metric = float(primary_metric) if primary_metric is not None else 0.0
        cv_mean = float(model.get('cv_mean', 0)) if model.get('cv_mean') is not None else 0.0
        cv_std = float(model.get('cv_std', 0)) if model.get('cv_std') is not None else 0.0
        status = "â­ Best" if idx == 0 else "Available"
        
        report_lines.append(f"| {model_name} | {primary_metric:.4f} | {cv_mean:.4f} | {cv_std:.4f} | {status} |")
    
    report_lines.append("\n---\n")
    
    # Detailed Model Explanations
    report_lines.append("## Detailed Model Analysis\n")
    
    for idx, model in enumerate(all_models):
        model_name = model.get('model_name', 'Unknown').replace('_', ' ').title()
        is_best = idx == 0
        
        report_lines.append(f"### {model_name} {'â­ (Recommended)' if is_best else ''}\n")
        
        # Metrics
        metrics_dict = model.get('metrics', {})
        if metrics_dict:
            report_lines.append("#### Performance Metrics\n")
            for metric_name, value in metrics_dict.items():
                if value is not None:
                    try:
                        report_lines.append(f"- **{metric_name}:** {float(value):.4f}")
                    except (ValueError, TypeError):
                        report_lines.append(f"- **{metric_name}:** {value}")
            report_lines.append("")
        
        # Primary metric highlight
        if model.get('primary_metric') is not None:
            report_lines.append(f"**Primary Metric Score:** {float(model.get('primary_metric', 0)):.4f}\n")
        
        # Full Explanation (if available from LLM explainer)
        explanation = model.get('explanation', {})
        if explanation:
            if isinstance(explanation, dict):
                if explanation.get('explanation'):
                    report_lines.append("#### Analysis\n")
                    report_lines.append(explanation.get('explanation'))
                    report_lines.append("")
                
                if explanation.get('strengths'):
                    report_lines.append("#### Strengths\n")
                    report_lines.append(explanation.get('strengths'))
                    report_lines.append("")
                
                if explanation.get('recommendation'):
                    report_lines.append("#### Recommendations\n")
                    report_lines.append(explanation.get('recommendation'))
                    report_lines.append("")
            elif isinstance(explanation, str):
                report_lines.append("#### Analysis\n")
                report_lines.append(explanation)
                report_lines.append("")
        
        # CV Scores
        if model.get('cv_scores'):
            report_lines.append("#### Cross-Validation Scores\n")
            cv_scores = model.get('cv_scores', [])
            report_lines.append(f"Individual fold scores: {', '.join([f'{s:.4f}' for s in cv_scores])}")
            report_lines.append(f"Mean: {model.get('cv_mean', 0):.4f} Â± {model.get('cv_std', 0):.4f}")
            report_lines.append("")
        
        # Feature Importance (if available)
        if model.get('feature_importance'):
            report_lines.append("#### Top Feature Importance\n")
            feat_imp = model.get('feature_importance', {})
            sorted_features = sorted(feat_imp.items(), key=lambda x: x[1], reverse=True)[:10]
            for feat, imp in sorted_features:
                report_lines.append(f"- **{feat}:** {imp:.4f}")
            report_lines.append("")
        
        report_lines.append("---\n")
    
    # Conclusion
    report_lines.append("## Conclusion\n")
    if all_models:
        best_model = all_models[0]
        best_name = best_model.get('model_name', 'Unknown').replace('_', ' ').title()
        report_lines.append(f"The **{best_name}** model performed best for this dataset and task.")
        report_lines.append("Consider the detailed analysis above when making deployment decisions.")
        report_lines.append("\nFor questions or further analysis, refer to the generated Jupyter notebook.")
    
    return "\n".join(report_lines)


def _generate_metrics_evaluation_code(plan, task: str) -> List[str]:
    """Generate metrics evaluation code from AutoMLPlan."""
    lines = []
    
    primary = plan.primary_metric
    additional = plan.additional_metrics
    
    is_classification = "classification" in task
    
    # Primary metric - handle variations like "f1_score_macro", "f1", etc.
    if is_classification:
        if primary == "accuracy" or "accuracy" in primary.lower():
            lines.append(f"primary_score = accuracy_score(y_test, y_pred)\n")
            lines.append(f"print(f'Primary Metric (Accuracy): {{primary_score:.4f}}')\n")
        elif "precision" in primary.lower():
            lines.append(f"from sklearn.metrics import precision_score\n")
            avg = "macro" if "macro" in primary.lower() else ("micro" if "micro" in primary.lower() else "weighted")
            lines.append(f"primary_score = precision_score(y_test, y_pred, average='{avg}', zero_division=0)\n")
            lines.append(f"print(f'Primary Metric (Precision {avg}): {{primary_score:.4f}}')\n")
        elif "recall" in primary.lower():
            lines.append(f"from sklearn.metrics import recall_score\n")
            avg = "macro" if "macro" in primary.lower() else ("micro" if "micro" in primary.lower() else "weighted")
            lines.append(f"primary_score = recall_score(y_test, y_pred, average='{avg}', zero_division=0)\n")
            lines.append(f"print(f'Primary Metric (Recall {avg}): {{primary_score:.4f}}')\n")
        elif "f1" in primary.lower():
            lines.append(f"from sklearn.metrics import f1_score\n")
            avg = "macro" if "macro" in primary.lower() else ("micro" if "micro" in primary.lower() else "weighted")
            lines.append(f"primary_score = f1_score(y_test, y_pred, average='{avg}', zero_division=0)\n")
            lines.append(f"print(f'Primary Metric (F1 {avg}): {{primary_score:.4f}}')\n")
        elif primary == "roc_auc":
            lines.append(f"from sklearn.metrics import roc_auc_score\n")
            lines.append(f"try:\n")
            lines.append(f"    y_pred_proba = pipeline.predict_proba(X_test)\n")
            lines.append(f"    if len(np.unique(y_test)) == 2:\n")
            lines.append(f"        primary_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n")
            lines.append(f"    else:\n")
            lines.append(f"        primary_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n")
            lines.append(f"    print(f'Primary Metric (ROC-AUC): {{primary_score:.4f}}')\n")
            lines.append(f"except Exception as e:\n")
            lines.append(f"    print(f'ROC-AUC not available: {{e}}')\n")
        
        # Additional metrics
        lines.append("\n# Additional metrics from plan:\n")
        for metric in additional:
            if metric == "precision" and primary != "precision":
                lines.append(f"print(f'Precision: {{precision_score(y_test, y_pred, average=\"macro\", zero_division=0):.4f}}')\n")
            elif metric == "recall" and primary != "recall":
                lines.append(f"print(f'Recall: {{recall_score(y_test, y_pred, average=\"macro\", zero_division=0):.4f}}')\n")
            elif metric == "f1" and primary not in ["f1", "f1_score"]:
                lines.append(f"print(f'F1: {{f1_score(y_test, y_pred, average=\"macro\", zero_division=0):.4f}}')\n")
        
        lines.append("\nprint(classification_report(y_test, y_pred))\n")
    else:
        # Regression
        if primary == "rmse":
            lines.append(f"from sklearn.metrics import mean_squared_error\n")
            lines.append(f"primary_score = np.sqrt(mean_squared_error(y_test, y_pred))\n")
            lines.append(f"print(f'Primary Metric (RMSE): {{primary_score:.4f}}')\n")
        elif primary == "mae":
            lines.append(f"from sklearn.metrics import mean_absolute_error\n")
            lines.append(f"primary_score = mean_absolute_error(y_test, y_pred)\n")
            lines.append(f"print(f'Primary Metric (MAE): {{primary_score:.4f}}')\n")
        elif primary in ["r2", "r2_score"]:
            lines.append(f"primary_score = r2_score(y_test, y_pred)\n")
            lines.append(f"print(f'Primary Metric (RÂ²): {{primary_score:.4f}}')\n")
        
        # Additional metrics
        lines.append("\n# Additional metrics from plan:\n")
        for metric in additional:
            if metric == "mae" and primary != "mae":
                lines.append(f"print(f'MAE: {{mean_absolute_error(y_test, y_pred):.4f}}')\n")
            elif metric in ["r2", "r2_score"] and primary not in ["r2", "r2_score"]:
                lines.append(f"print(f'RÂ²: {{r2_score(y_test, y_pred):.4f}}')\n")
            elif metric == "rmse" and primary != "rmse":
                lines.append(f"print(f'RMSE: {{np.sqrt(mean_squared_error(y_test, y_pred)):.4f}}')\n")
    
    return lines
