"""
Generate artifacts: Jupyter notebook, pickle file, charts, README
"""

import json
import pickle
import base64
from typing import Dict, Any, Optional
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import numpy as np


def generate_notebook(
    df: pd.DataFrame,
    target: str,
    task: str,
    config: Dict[str, Any],
    metrics: Dict[str, float],
    feature_importance: Optional[Dict[str, float]] = None,
    model: Any = None
) -> str:
    """
    Generate a Jupyter notebook with complete training code.
    """
    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    f"# ML Model Training: Predicting {target}\n",
                    f"\n",
                    f"This notebook was auto-generated by Intent2Model.\n",
                    f"\n",
                    f"**Task:** {task}\n",
                    f"**Target Column:** {target}\n",
                    f"**Model:** {config.get('model', 'unknown')}\n"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 1. Import Libraries"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "from sklearn.model_selection import train_test_split, cross_val_score\n",
                    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
                    "from sklearn.impute import SimpleImputer\n",
                    "from sklearn.pipeline import Pipeline\n",
                    "from sklearn.compose import ColumnTransformer\n",
                    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
                    "import pickle\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "\n",
                    "# Set style\n",
                    "sns.set_style('whitegrid')\n",
                    "plt.rcParams['figure.figsize'] = (12, 6)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 2. Load Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load your dataset\n",
                    "# df = pd.read_csv('your_dataset.csv')\n",
                    "\n",
                    "# For this example, we'll use the uploaded data\n",
                    f"print(f'Dataset shape: {{df.shape}}')\n",
                    f"print(f'Columns: {{list(df.columns)}}')\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 3. Prepare Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        f"# Separate features and target\n",
                        f"X = df.drop(columns=['{target}'])\n",
                        f"y = df['{target}']\n",
                        "\n",
                        "# Handle categorical target if needed\n",
                    ] + 
                    (["le = LabelEncoder()\n", "y = le.fit_transform(y.astype(str))\n"] if task == 'classification' else []) +
                    [
                        "\n",
                        "# Split data\n",
                        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                        "\n",
                        "print(f'Training set: {X_train.shape}')\n",
                        "print(f'Test set: {X_test.shape}')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 4. Build Pipeline"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Identify numeric and categorical columns\n",
                    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                    "\n",
                    "# Create preprocessing steps\n",
                    "numeric_transformer = Pipeline(steps=[\n",
                    "    ('imputer', SimpleImputer(strategy='mean')),\n",
                    "    ('scaler', StandardScaler())\n",
                    "])\n",
                    "\n",
                    "categorical_transformer = Pipeline(steps=[\n",
                    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                    "])\n",
                    "\n",
                    "# Combine preprocessing\n",
                    "preprocessor = ColumnTransformer(\n",
                    "    transformers=[\n",
                    "        ('num', numeric_transformer, numeric_cols),\n",
                    "        ('cat', categorical_transformer, categorical_cols)\n",
                    "    ]\n",
                    ")\n",
                    "\n",
                    f"# Create model\n",
                    f"{'model = RandomForestClassifier(n_estimators=100, random_state=42)' if task == 'classification' else 'model = RandomForestRegressor(n_estimators=100, random_state=42)'}\n",
                    "\n",
                    "# Create full pipeline\n",
                ] + (
                    ["model = RandomForestClassifier(n_estimators=100, random_state=42)\n"]
                    if task == 'classification'
                    else ["model = RandomForestRegressor(n_estimators=100, random_state=42)\n"]
                ) + [
                    "\n",
                    "pipeline = Pipeline(steps=[\n",
                    "    ('preprocessor', preprocessor),\n",
                    "    ('model', model)\n",
                    "])"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 5. Train Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Train the model\n",
                        "pipeline.fit(X_train, y_train)\n",
                        "\n",
                        "# Make predictions\n",
                        "y_pred = pipeline.predict(X_test)\n",
                        "\n",
                        "# Evaluate\n",
                    ] + (
                        ["score = accuracy_score(y_test, y_pred)\n", "print(f'Accuracy: {score:.4f}')\n", "\n", "print(classification_report(y_test, y_pred))"] 
                        if task == 'classification' 
                        else ["score = r2_score(y_test, y_pred)\n", "print(f'R2 Score: {score:.4f}')\n", "\n", "print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')"]
                    )
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 6. Feature Importance"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Get feature importance\n",
                    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
                    "    importances = pipeline.named_steps['model'].feature_importances_\n",
                    "    feature_names = numeric_cols + categorical_cols\n",
                    "    \n",
                    "    # Create importance DataFrame\n",
                    "    importance_df = pd.DataFrame({\n",
                    "        'feature': feature_names[:len(importances)],\n",
                    "        'importance': importances\n",
                    "    }).sort_values('importance', ascending=False)\n",
                    "    \n",
                    "    # Plot\n",
                    "    plt.figure(figsize=(10, 6))\n",
                    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
                    "    plt.title('Top 10 Feature Importance')\n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "    \n",
                    "    print(importance_df)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 7. Save Model"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": (
                    [
                        "# Save the trained model\n",
                        "with open('model.pkl', 'wb') as f:\n",
                        "    pickle.dump(pipeline, f)\n",
                    ] + (
                        ["\n", "# Save label encoder if used\n", "with open('label_encoder.pkl', 'wb') as f:\n", "    pickle.dump(le, f)\n"]
                        if task == 'classification'
                        else []
                    ) + [
                        "\n",
                        "print('Model saved successfully!')"
                    ]
                )
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## 8. Make Predictions"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load model for predictions\n",
                    "# with open('model.pkl', 'rb') as f:\n",
                    "#     loaded_model = pickle.load(f)\n",
                    "\n",
                    "# Example prediction\n",
                    "# new_data = pd.DataFrame({...})\n",
                    "# prediction = loaded_model.predict(new_data)\n",
                    "# print(f'Prediction: {prediction}')"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.10.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    return json.dumps(notebook, indent=1)


def generate_readme(
    target: str,
    task: str,
    metrics: Dict[str, float],
    config: Dict[str, Any],
    feature_importance: Optional[Dict[str, float]] = None,
    dataset_info: Optional[Dict[str, Any]] = None
) -> str:
    """
    Generate a comprehensive README.md file.
    """
    readme = f"""# ML Model: Predicting {target}

This model was auto-generated by **Intent2Model** - an LLM-guided AutoML platform.

## ðŸ“Š Model Overview

- **Task Type:** {task.title()}
- **Target Column:** `{target}`
- **Model Architecture:** {config.get('model', 'Unknown')}
- **Preprocessing:** {', '.join(config.get('preprocessing', [])) or 'None'}

## ðŸ“ˆ Performance Metrics

"""
    
    for metric_name, metric_value in metrics.items():
        readme += f"- **{metric_name.upper()}:** {metric_value:.4f}\n"
    
    readme += f"""
## ðŸ”§ Model Details

### Preprocessing Steps
"""
    
    preprocessing = config.get('preprocessing', [])
    if preprocessing:
        for step in preprocessing:
            readme += f"- {step.replace('_', ' ').title()}\n"
    else:
        readme += "- No preprocessing applied\n"
    
    readme += f"""
### Model Configuration
- **Algorithm:** {config.get('model', 'Unknown')}
- **Task:** {task}

## ðŸ“¦ Files Included

- `model.pkl` - Trained model (pickle format)
- `training_notebook.ipynb` - Complete Jupyter notebook with training code
- `README.md` - This file
- `charts/` - Visualization charts (if generated)

## ðŸš€ Usage

### Load and Use the Model

```python
import pickle
import pandas as pd

# Load the model
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

# Prepare your data (same format as training data)
new_data = pd.DataFrame({{
    # Your feature columns here
}})

# Make prediction
prediction = model.predict(new_data)
print(f'Prediction: {{prediction}}')
```

## ðŸ“Š Feature Importance

"""
    
    if feature_importance:
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        readme += "Top features by importance:\n\n"
        for i, (feature, importance) in enumerate(sorted_features[:10], 1):
            readme += f"{i}. **{feature}**: {importance:.4f}\n"
    else:
        readme += "Feature importance not available.\n"
    
    readme += f"""
## ðŸ”„ Retraining

To retrain the model, use the provided Jupyter notebook (`training_notebook.ipynb`).

## ðŸ“ Notes

- Model was trained using cross-validation
- All preprocessing steps are included in the pipeline
- The model can be used directly for predictions

## ðŸ¤– Generated by Intent2Model

This model was automatically generated by Intent2Model's LLM-guided AutoML system.
For more information, visit: https://github.com/lakshitsachdeva/intent2model

---
*Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return readme


def save_model_pickle(model: Any, filepath: str) -> str:
    """
    Save model as pickle file and return base64 encoded string.
    """
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    
    with open(filepath, 'rb') as f:
        return base64.b64encode(f.read()).decode('utf-8')


def generate_chart_image(
    chart_type: str,
    data: Dict[str, Any],
    title: str = ""
) -> str:
    """
    Generate chart as base64 encoded image.
    """
    plt.figure(figsize=(10, 6))
    
    if chart_type == "metrics":
        metrics_data = data.get("data", [])
        names = [d["name"] for d in metrics_data]
        values = [d["value"] for d in metrics_data]
        
        plt.bar(names, values, color='steelblue')
        plt.title(title or "Performance Metrics")
        plt.ylabel("Score")
        plt.xticks(rotation=45, ha='right')
        
    elif chart_type == "feature_importance":
        features = list(data.keys())[:10]
        importances = [data[f] for f in features]
        
        plt.barh(features, importances, color='coral')
        plt.title(title or "Feature Importance")
        plt.xlabel("Importance")
        
    elif chart_type == "cv_scores":
        scores = data.get("scores", [])
        folds = [f"Fold {i+1}" for i in range(len(scores))]
        
        plt.plot(folds, scores, marker='o', linewidth=2, markersize=8)
        plt.title(title or "Cross-Validation Scores")
        plt.ylabel("Score")
        plt.xlabel("Fold")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Convert to base64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')
    buffer.seek(0)
    image_base64 = base64.b64encode(buffer.read()).decode('utf-8')
    plt.close()
    
    return image_base64
