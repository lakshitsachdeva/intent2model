{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training: Predicting variety\n",
    "\n",
    "This notebook was auto-generated by Intent2Model.\n",
    "\n",
    "**Task:** classification\n",
    "**Target Column:** variety\n",
    "**Model:** logistic_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANNING SOURCE\n\n**Planning Method:** LLM\n\n**Target Confidence:** 1.00\n**Task Confidence:** 1.00\n**Plan Quality:** High Confidence\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0 \u2014 TASK INFERENCE\n\n### Step 0: Task Inference\n\n*   **Target Variable**: The user specified `variety` as the target. This column is present in the dataset.\n*   **Task Type**: The target column `variety` is of `object` dtype and has 3 unique values ('Setosa', 'Versicolor', 'Virginica'). The features are all numeric. This is a classic **multiclass classification** problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 \u2014 DATASET INTELLIGENCE\n\n### Step 1: Dataset Intelligence\n\n*   **Shape**: The dataset is small, with 150 rows and 5 columns.\n*   **Data Types**: There are 4 numeric features (`float64`) and 1 categorical target (`object`).\n*   **Missingness**: There are no missing values in any column, which simplifies preprocessing.\n*   **Feature Analysis**:\n    *   The numeric features (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`) have low skewness values, indicating they are fairly symmetric.\n    *   The value ranges are not dramatically different, but scaling is still recommended for distance-based or gradient-based models.\n    *   There is a very minor outlier hint in `sepal.width` based on the IQR method, but it's not significant enough to warrant complex outlier removal for this initial plan.\n*   **Target Analysis**: The target variable `variety` is perfectly balanced, with each of the 3 classes having exactly 50 samples. This is ideal and means we don't need to employ class imbalance handling techniques like SMOTE or class weights. Accuracy can be used as a reliable primary metric.\n*   **Identifiers/Leakage**: No columns with high cardinality or other signs of being identifiers were detected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 \u2014 TRANSFORMATION STRATEGY\n\n### Step 2: Transformation Strategy\n\n*   **Numeric Features (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`)**: \n    *   **Decision**: `StandardScaler`.\n    *   **Justification**: While not strictly necessary for tree-based models, scaling is critical for models like Logistic Regression, SVM, and KNN to prevent features with larger scales from dominating the model's learning process. `StandardScaler` is a robust choice that centers the data to a mean of 0 and scales to a standard deviation of 1, making it suitable for all planned model candidates.\n\n*   **Target Feature (`variety`)**: \n    *   **Decision**: `Drop`.\n    *   **Justification**: This is the target variable (y) and must be excluded from the feature matrix (X) to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 \u2014 MODEL CANDIDATE SELECTION\n\n### Step 3: Model Selection\n\nGiven the small, clean, and balanced nature of the dataset, a few well-established models are sufficient.\n\n*   **`LogisticRegression`**: \n    *   **Justification**: Included as a simple, fast, and highly interpretable linear baseline. It performs well when the decision boundary is roughly linear.\n*   **`RandomForestClassifier`**: \n    *   **Justification**: Included as a powerful, non-linear ensemble model. It is robust to the data's scale (though scaling doesn't hurt) and captures complex interactions between features. It also provides feature importances for free.\n*   **`SupportVectorClassifier` (SVC)**: \n    *   **Justification**: Included because SVMs are highly effective in high-dimensional space and are excellent at finding clear separation margins, a known characteristic of the Iris dataset. Performance is dependent on proper scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 \u2014 TRAINING & VALIDATION\n\n### Step 4: Training & Validation Plan\n\n*   **Validation Strategy**: `Stratified 5-Fold Cross-Validation`.\n    *   **Justification**: The dataset is too small (150 rows) for a simple train-test split to be reliable. K-fold cross-validation provides a more robust estimate of model performance. `Stratified` k-fold is chosen to ensure that the 50/50/50 class balance is preserved in each fold, which is best practice for classification tasks.\n*   **Performance Metrics**:\n    *   **Primary Metric**: `accuracy`.\n        *   **Justification**: The classes are perfectly balanced, making accuracy a direct and easy-to-interpret measure of overall correctness.\n    *   **Additional Metrics**: `f1_macro`, `roc_auc_ovr`.\n        *   **Justification**: `f1_macro` calculates the F1-score for each class and finds their unweighted mean, which is a good holistic measure for multiclass problems. `roc_auc_ovr` (One-vs-Rest) provides a measure of the model's ability to discriminate between each class and the rest, which is useful for understanding per-class performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 \u2014 ERROR & BEHAVIOR ANALYSIS\n\n### Step 5: Error/Behavior Analysis Plan\n\nTo understand model performance beyond single metrics, the following analyses are critical:\n\n*   **Confusion Matrix**: This is the most important tool for a classification task. It will be plotted to visualize which classes are being confused for which (e.g., is the model struggling to distinguish between 'Versicolor' and 'Virginica'?).\n*   **Classification Report**: This will be generated to provide a per-class breakdown of precision, recall, and F1-score, giving a detailed view of performance for each of the three flower species.\n*   **Feature Importance Plot**: For the `RandomForestClassifier`, a bar plot of feature importances will be created to show which features (e.g., 'petal.width') are most influential in the model's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 \u2014 EXPLAINABILITY\n\n### Step 6: Explainability Plan\n\n*   **Feature Name Alignment**: The feature transformations consist only of scaling. The post-transformation feature names will directly correspond to the original features (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`), so no complex name mapping is required.\n*   **Explainability Method**: The primary method will be analyzing the `feature_importances_` attribute from the trained `RandomForestClassifier`. \n    *   **Justification**: This provides a straightforward, global explanation of which features are driving the model's predictions on average. For a simple dataset like this, it is often sufficient to understand the key drivers of the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For this example, we'll use the uploaded data\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['variety'])\n",
    "y = df['variety']\n",
    "\n",
    "# Handle categorical target if needed\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y.astype(str))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Preprocessing Pipeline (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing compiled from AutoMLPlan\n# Each feature transform is based on plan.feature_transforms\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\n\ntransformers = []\n\n# Create preprocessor from plan-driven transformers\n\nif len(transformers) == 0:\n    # \u26a0\ufe0f WARNING: No transformers generated from plan.feature_transforms! Using runtime fallback.\n    numeric_cols = ['sepal.length', 'sepal.width', 'petal.length', 'petal.width']\n    transformers.append(('num_scaled', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numeric_cols))\npreprocessor = ColumnTransformer(transformers, remainder='drop')\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Model (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model compiled from AutoMLPlan.model_candidates\n\n# Selected model: logistic_regression (from plan.model_candidates)\n# Reason: Selected based on plan recommendations.\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000, random_state=42)\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assemble Pipeline (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Assemble pipeline from plan-driven components\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate using metrics from AutoMLPlan\n",
    "# Metrics compiled from AutoMLPlan\n# Primary metric: accuracy\n# Additional metrics: ['f1_macro', 'roc_auc_ovr']\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report\n)\n\n",
    "\n",
    "# Calculate metrics\n",
    "primary_score = accuracy_score(y_test, y_pred)\n",
    "print(f'Primary Metric (Accuracy): {primary_score:.4f}')\n",
    "\n# Additional metrics from plan:\n",
    "\nprint(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (from plan.explainability_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature importance (aligned with plan)\n",
    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    importances = pipeline.named_steps['model'].feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing (aligned with plan, not dtype-based)\n",
    "    # NOTE: Do NOT use numeric_cols or categorical_cols - they may not be defined\n",
    "    try:\n",
    "        preprocessor = pipeline.named_steps['preprocessor']\n",
    "        feature_names = []\n",
    "        # Get feature names from preprocessor transformers\n",
    "        if hasattr(preprocessor, 'transformers_'):\n",
    "            for name, transformer, cols in preprocessor.transformers_:\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    feature_names.extend(transformer.get_feature_names_out(cols))\n",
    "                elif hasattr(transformer, 'named_steps'):\n",
    "                    # Pipeline transformer\n",
    "                    for step_name, step_transformer in transformer.named_steps.items():\n",
    "                        if hasattr(step_transformer, 'get_feature_names_out'):\n",
    "                            feature_names.extend(step_transformer.get_feature_names_out(cols))\n",
    "                            break\n",
    "                else:\n",
    "                    # Fallback: use column names\n",
    "                    feature_names.extend([f'{name}_{col}' for col in cols])\n",
    "        else:\n",
    "            # Preprocessor not fitted yet - use generic names\n",
    "            feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
    "    except Exception as e:\n",
    "        # Fallback: use generic feature names\n",
    "        feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(importances)],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(importance_df)\n",
    "else:\n",
    "    print('Feature importance not available for this model type.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the trained model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Save label encoder if used\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model for predictions\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "\n",
    "# Example prediction\n",
    "# new_data = pd.DataFrame({...})\n",
    "# prediction = loaded_model.predict(new_data)\n",
    "# print(f'Prediction: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}