{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training: Predicting variety\n",
    "\n",
    "This notebook was auto-generated by Intent2Model.\n",
    "\n",
    "**Task:** classification\n",
    "**Target Column:** variety\n",
    "**Model:** logistic_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANNING SOURCE\n",
    "\n",
    "⚠️ **LOW-CONFIDENCE FALLBACK PLAN**\n",
    "\n",
    "This plan was generated using rule-based fallbacks because the LLM was unavailable or returned invalid responses. **Results may be suboptimal.**\n",
    "\n",
    "**Planning Method:** LLM\n",
    "\n",
    "**Target Confidence:** 1.00\n",
    "**Task Confidence:** 1.00\n",
    "**Plan Quality:** Fallback Low Confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0 — TASK INFERENCE\n",
    "\n",
    "The requested target column is `variety`. \n",
    "\n",
    "Inspecting the dataset profile, the `variety` column has a `str` data type and `nunique` of 3, with values \"Setosa\", \"Versicolor\", and \"Virginica\". These are distinct, nominal categories. \n",
    "\n",
    "Since the target is a categorical variable with more than two unique values, the task is inferred as **Multiclass Classification**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 — DATASET INTELLIGENCE\n",
    "\n",
    "The dataset contains 150 rows and 5 columns. It is a relatively small dataset.\n",
    "\n",
    "**Feature Classification & Dtypes:**\n",
    "*   `sepal.length`, `sepal.width`, `petal.length`, `petal.width`: These are numerical features, all `float64`. They represent continuous measurements.\n",
    "*   `variety`: This is the target column, a categorical `str` type.\n",
    "\n",
    "**Missingness:**\n",
    "There are no missing values in any of the columns (`missing_percent` is 0.0 for all features). This simplifies the preprocessing step as no imputation is required.\n",
    "\n",
    "**Uniques & Cardinality:**\n",
    "*   Numerical features have reasonable `nunique` values (35-43) relative to the 150 rows, suggesting they are not identifiers and have sufficient variance.\n",
    "*   The target `variety` has 3 unique values, confirming the multiclass nature.\n",
    "\n",
    "**Distributions & Skewness:**\n",
    "*   `sepal.length` (skew: 0.31), `sepal.width` (skew: 0.32), `petal.length` (skew: -0.27), `petal.width` (skew: -0.10). All numerical features exhibit relatively low skewness (close to 0), suggesting approximately symmetrical distributions. No strong indication for non-linear transformations like log or power transforms.\n",
    "\n",
    "**Outliers:**\n",
    "*   `sepal.width` shows a minimal `outlier_hint_iqr` of 0.027. All other numerical features show 0.0. This indicates that outliers are either non-existent or very minor, not necessitating robust scaling strategies for now.\n",
    "\n",
    "**Imbalance:**\n",
    "*   The target column `variety` shows perfect balance with 50 counts for each of its 3 classes (\"Setosa\", \"Versicolor\", \"Virginica\"). This simplifies model training and evaluation, as standard metrics can be used without explicit handling for imbalance.\n",
    "\n",
    "**Leakage/Identifiers:**\n",
    "*   No columns were identified as `identifier_like_columns`. There are no apparent features that would directly leak information about the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 — TRANSFORMATION STRATEGY\n",
    "\n",
    "**Overall Strategy:** Given the clean nature of the dataset (no missing values, no strong skew, minor outliers), the transformation strategy will be straightforward, focusing on preparing numerical features for various model types and handling the target.\n",
    "\n",
    "**Per-Feature Decisions:**\n",
    "\n",
    "*   **`sepal.length` (Numerical Feature):**\n",
    "    *   `impute_strategy`: None, as there are no missing values.\n",
    "    *   `encoding_strategy`: Not applicable, as it's a numerical feature.\n",
    "    *   `scaling_strategy`: **Standard Scaling**. \n",
    "        *   **Justification:** While tree-based models are not sensitive to scaling, many other common models (e.g., Logistic Regression, SVM, KNN) perform better or converge faster with scaled features. Given the low skewness and minimal outliers, StandardScaler is a suitable choice to bring features to a comparable scale (mean 0, variance 1) without distorting the distribution significantly.\n",
    "\n",
    "*   **`sepal.width` (Numerical Feature):**\n",
    "    *   `impute_strategy`: None, as there are no missing values.\n",
    "    *   `encoding_strategy`: Not applicable, as it's a numerical feature.\n",
    "    *   `scaling_strategy`: **Standard Scaling**. \n",
    "        *   **Justification:** Same as `sepal.length`. Despite a minimal outlier hint, StandardScaler should be robust enough for this slight deviation, ensuring consistent scaling across all numerical features.\n",
    "\n",
    "*   **`petal.length` (Numerical Feature):**\n",
    "    *   `impute_strategy`: None, as there are no missing values.\n",
    "    *   `encoding_strategy`: Not applicable, as it's a numerical feature.\n",
    "    *   `scaling_strategy`: **Standard Scaling**. \n",
    "        *   **Justification:** Same as `sepal.length`.\n",
    "\n",
    "*   **`petal.width` (Numerical Feature):**\n",
    "    *   `impute_strategy`: None, as there are no missing values.\n",
    "    *   `encoding_strategy`: Not applicable, as it's a numerical feature.\n",
    "    *   `scaling_strategy`: **Standard Scaling**. \n",
    "        *   **Justification:** Same as `sepal.length`.\n",
    "\n",
    "*   **`variety` (Target Column):**\n",
    "    *   `drop`: **True**. \n",
    "        *   **Justification:** This is the target variable and should be separated from the feature set used for training. It will be used as the ground truth for model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 — MODEL CANDIDATE SELECTION\n",
    "\n",
    "Given that this is a multiclass classification task on a small dataset (150 rows) with 4 numerical features, the selection prioritizes models known for good performance on such data, computational efficiency, and a balance between interpretability and predictive power.\n",
    "\n",
    "**Included Models:**\n",
    "\n",
    "*   **Logistic Regression:**\n",
    "    *   **Reason:** A strong, interpretable baseline model. It's efficient, works well on small datasets, and performs effectively on linearly separable data. It benefits from feature scaling, which is planned.\n",
    "    *   **Configuration:** Uses `lbfgs` solver for multiclass classification ('multinomial').\n",
    "\n",
    "*   **Random Forest Classifier:**\n",
    "    *   **Reason:** An ensemble tree-based model known for its robustness, ability to capture non-linear relationships, and handling of feature interactions. It generally performs well across various datasets and is less sensitive to feature scaling (though scaling won't hurt). Good for small to medium-sized datasets.\n",
    "    *   **Configuration:** Standard number of estimators (e.g., 100).\n",
    "\n",
    "*   **Support Vector Machine (SVC) with Radial Basis Function (RBF) Kernel:**\n",
    "    *   **Reason:** Powerful and versatile, particularly effective in classification tasks, even with a relatively small number of features. The RBF kernel allows it to model complex non-linear decision boundaries. It is sensitive to feature scaling, which is addressed in the transformation step.\n",
    "    *   **Configuration:** A common kernel (rbf).\n",
    "\n",
    "*   **Gradient Boosting Classifier (LightGBM):**\n",
    "    *   **Reason:** Highly performant, state-of-the-art tree-boosting algorithm. Known for its speed and accuracy, even on smaller datasets. Can model complex relationships and interactions. Offers excellent predictive power.\n",
    "    *   **Configuration:** Default parameters are usually a good starting point.\n",
    "\n",
    "*   **K-Nearest Neighbors Classifier:**\n",
    "    *   **Reason:** A simple, instance-based learning algorithm that can perform very well on small, well-separated datasets. It's highly interpretable in terms of local neighborhoods and sensitive to feature scaling.\n",
    "    *   **Configuration:** A typical number of neighbors (e.g., 5).\n",
    "\n",
    "**Excluded Models & Justification:**\n",
    "\n",
    "*   **Deep Learning Models (e.g., Neural Networks):**\n",
    "    *   **Reason for Exclusion:** For a dataset of only 150 rows and 4 features, deep learning models are generally overkill. They typically require much larger datasets to justify their complexity and achieve superior performance, and are prone to overfitting on small data. Simpler, more traditional ML models often perform comparably or better with less computational overhead and easier interpretability in such scenarios.\n",
    "\n",
    "*   **Naïve Bayes:**\n",
    "    *   **Reason for Exclusion:** While a valid classifier, its strong independence assumption between features might be too restrictive. Models like Logistic Regression and SVM offer more flexibility and often better performance without this strict assumption, especially with scaled numerical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 — TRAINING & VALIDATION\n",
    "\n",
    "**Cross-Validation Strategy:**\n",
    "\n",
    "*   **Method:** **Stratified K-Fold Cross-Validation** (e.g., K=5 or K=10).\n",
    "    *   **Justification:** Given the small dataset size (150 rows), a robust validation strategy is crucial to get reliable performance estimates and reduce variance. K-Fold CV ensures that each data point is used for both training and validation across different folds. Stratification is essential for classification tasks, especially with multiple classes, to ensure that each fold maintains the same proportion of classes as the original dataset. This is particularly important for multiclass problems, even with balanced classes, to prevent accidental splits that could lead to missing classes in a fold.\n",
    "\n",
    "**Metrics:**\n",
    "\n",
    "*   **Task Type:** Multiclass Classification.\n",
    "*   **Dataset Characteristics:** Perfectly balanced target classes.\n",
    "\n",
    "*   **Primary Metric: `accuracy_score`**\n",
    "    *   **Justification:** Since the classes are perfectly balanced (50 samples for each of the 3 classes), accuracy is a straightforward and appropriate primary metric. It directly reflects the overall proportion of correctly classified instances.\n",
    "\n",
    "*   **Additional Metrics:**\n",
    "    *   `f1_score_macro`: \n",
    "        *   **Justification:** While accuracy is good for balanced classes, F1-score provides a harmonic mean of precision and recall. Using `macro` averaging is suitable here because it calculates the metric independently for each class and then takes the unweighted average, treating all classes equally. This aligns with the balanced nature of the dataset.\n",
    "    *   `precision_score_macro`: \n",
    "        *   **Justification:** Macro-averaged precision helps evaluate the models' ability to avoid false positives across all classes, giving equal weight to each class.\n",
    "    *   `recall_score_macro`: \n",
    "        *   **Justification:** Macro-averaged recall helps evaluate the models' ability to find all positive samples for each class, giving equal weight to each class.\n",
    "    *   `confusion_matrix`: \n",
    "        *   **Justification:** Essential for multiclass classification. It provides a detailed breakdown of correct and incorrect predictions for each class, showing which classes are being confused with others. This qualitative insight is invaluable.\n",
    "\n",
    "**Overfitting Checks:**\n",
    "\n",
    "*   **Cross-Validation:** The K-Fold cross-validation strategy inherently helps in detecting overfitting. If a model performs significantly better on the training folds than on the validation folds, it's an indication of overfitting.\n",
    "*   **Score Comparison:** We will monitor both training set performance (averaged across folds) and validation set performance (averaged across folds) for each metric. A substantial gap between training and validation scores will signal overfitting.\n",
    "*   **Learning Curves:** For more in-depth analysis, plotting learning curves (model performance vs. training set size) can reveal if the model is suffering from high bias (underfitting) or high variance (overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 — ERROR & BEHAVIOR ANALYSIS\n",
    "\n",
    "For a multiclass classification task, understanding where the model makes mistakes and why is critical. The analysis plan focuses on actionable insights:\n",
    "\n",
    "1.  **Confusion Matrix Analysis:**\n",
    "    *   **Justification:** This is fundamental for multiclass problems. It will show the counts of true positives, false positives, true negatives, and false negatives for each class. We will analyze the matrix to identify which specific classes are most often confused with each other (e.g., 'Versicolor' misclassified as 'Virginica' and vice-versa), providing direct insights into classification errors.\n",
    "\n",
    "2.  **Classification Report (Per-Class Metrics):**\n",
    "    *   **Justification:** Complementing the confusion matrix, a classification report will provide precision, recall, and F1-score for each individual class. This helps pinpoint if the model struggles more with predicting certain classes (low recall) or if it frequently mislabels other classes as a specific one (low precision for that class).\n",
    "\n",
    "3.  **Misclassified Samples Inspection:**\n",
    "    *   **Justification:** Select a subset of samples that were consistently misclassified across folds or by the best-performing models. Analyze their feature values (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`) to look for common patterns, boundary conditions, or unusual feature combinations that might explain the misclassification. This can reveal edge cases or areas where features might overlap between classes.\n",
    "\n",
    "4.  **Feature Importance Analysis (for tree-based models like Random Forest, LightGBM):**\n",
    "    *   **Justification:** Investigate which features contribute most to the models' decisions. This can reveal if certain features are more discriminative than others, and if the model is relying on expected features (e.g., petal measurements are often more discriminative for Iris species than sepal measurements).\n",
    "\n",
    "5.  **ROC Curves and AUC for Multiclass (One-vs-Rest):**\n",
    "    *   **Justification:** Although traditionally binary, ROC curves can be extended to multiclass by using a one-vs-rest (OvR) approach. Plotting OvR ROC curves for each class helps assess the model's ability to distinguish each class from all others, and the AUC score provides a summarized measure of this separability across different probability thresholds. This helps evaluate the probabilistic outputs of the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 — EXPLAINABILITY\n",
    "\n",
    "Given the small, structured dataset and the selection of models, explainability will focus on understanding feature importance and individual prediction contributions. Since all features are numerical and directly used (after scaling), post-encoding feature name alignment is straightforward.\n",
    "\n",
    "**Feature Name Alignment:**\n",
    "*   The original numerical feature names (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`) will remain consistent throughout the preprocessing (scaling) and modeling steps. No categorical features are being one-hot encoded into multiple new columns, so there's no need to map encoded feature names back to original categories.\n",
    "*   Explainability tools will directly use these four original feature names, making the explanations clear and interpretable.\n",
    "\n",
    "**Explainability Techniques:**\n",
    "\n",
    "1.  **SHAP (SHapley Additive exPlanations):**\n",
    "    *   **Justification:** SHAP values are a powerful, model-agnostic technique derived from cooperative game theory that provides consistent and locally accurate explanations. They quantify how much each feature contributes to an individual prediction (local explainability) and can also be aggregated to understand overall feature importance (global explainability).\n",
    "    *   **Application:** We will use SHAP to explain specific misclassifications (local explanations) to understand why a particular instance was predicted incorrectly. We will also generate global SHAP summary plots to understand the overall impact and direction of influence for each feature across the dataset.\n",
    "\n",
    "2.  **Permutation Importance:**\n",
    "    *   **Justification:** Permutation importance is a model-agnostic technique that measures the importance of a feature by quantifying how much the model's performance decreases when that feature's values are randomly shuffled (breaking its relationship with the target). It is robust and provides a reliable measure of global feature importance.\n",
    "    *   **Application:** This will be used to identify the most critical features for each trained model, providing a complementary perspective to SHAP for global feature importance. It helps validate if the model is using features in an intuitively correct way (e.g., 'petal.length' and 'petal.width' are expected to be highly important for Iris species classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For this example, we'll use the uploaded data\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['variety'])\n",
    "y = df['variety']\n",
    "\n",
    "# Handle categorical target if needed\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y.astype(str))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Build preprocessing dynamically from AutoML plan (if provided)\n",
    "feature_transforms = []\n",
    "transformers = []\n",
    "dropped = set([ft['name'] for ft in feature_transforms if ft.get('drop')])\n",
    "num_scaled = [ft['name'] for ft in feature_transforms if ft.get('name') in numeric_cols and ft.get('scale') == 'standard' and not ft.get('drop')]\n",
    "num_plain = [c for c in numeric_cols if c not in dropped and c not in num_scaled]\n",
    "cat_onehot = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'one_hot' and not ft.get('drop')]\n",
    "cat_ordinal = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'ordinal' and not ft.get('drop')]\n",
    "cat_freq = [ft['name'] for ft in feature_transforms if ft.get('name') in categorical_cols and ft.get('encode') == 'frequency' and not ft.get('drop')]\n",
    "\n",
    "if num_scaled:\n",
    "    transformers.append(('num_scaled', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_scaled))\n",
    "if num_plain:\n",
    "    transformers.append(('num', Pipeline([('imputer', SimpleImputer(strategy='median'))]), num_plain))\n",
    "if cat_onehot:\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=5)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    transformers.append(('cat_onehot', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', ohe)]), cat_onehot))\n",
    "if cat_ordinal:\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    transformers.append(('cat_ordinal', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))]), cat_ordinal))\n",
    "if cat_freq:\n",
    "    # Simple frequency encoding\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, X, y=None):\n",
    "            import numpy as np\n",
    "            X = np.asarray(X, dtype=object)\n",
    "            self.maps_ = []\n",
    "            for j in range(X.shape[1]):\n",
    "                col = [\"\" if v is None else str(v) for v in X[:, j]]\n",
    "                counts = {}\n",
    "                for v in col:\n",
    "                    counts[v] = counts.get(v, 0) + 1\n",
    "                n = float(max(1, len(col)))\n",
    "                self.maps_.append({k: c / n for k, c in counts.items()})\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            import numpy as np\n",
    "            X = np.asarray(X, dtype=object)\n",
    "            out = np.zeros((X.shape[0], X.shape[1]), dtype=float)\n",
    "            for j in range(X.shape[1]):\n",
    "                m = self.maps_[j]\n",
    "                col = [\"\" if v is None else str(v) for v in X[:, j]]\n",
    "                out[:, j] = [float(m.get(v, 0.0)) for v in col]\n",
    "            return out\n",
    "    transformers.append(('cat_freq', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('freq', FrequencyEncoder())]), cat_freq))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "# Create model (auto-generated for this dataset/run)\n",
    "model = LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {score:.4f}')\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    importances = pipeline.named_steps['model'].feature_importances_\n",
    "    feature_names = numeric_cols + categorical_cols\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(importances)],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Save label encoder if used\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for predictions\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "\n",
    "# Example prediction\n",
    "# new_data = pd.DataFrame({...})\n",
    "# prediction = loaded_model.predict(new_data)\n",
    "# print(f'Prediction: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
