{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training: Predicting variety\n",
    "\n",
    "This notebook was auto-generated by Intent2Model.\n",
    "\n",
    "**Task:** classification\n",
    "**Target Column:** variety\n",
    "**Model:** logistic_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLANNING SOURCE\n",
    "\n",
    "⚠️ **LOW-CONFIDENCE FALLBACK PLAN**\n",
    "\n",
    "This plan was generated using rule-based fallbacks because the LLM was unavailable or returned invalid responses. **Results may be suboptimal.**\n",
    "\n",
    "**Planning Method:** LLM\n",
    "\n",
    "**Target Confidence:** 1.00\n",
    "**Task Confidence:** 1.00\n",
    "**Plan Quality:** Fallback Low Confidence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0 — TASK INFERENCE\n",
    "\n",
    "The requested target column `variety` is identified as a categorical feature with three unique string values: 'Setosa', 'Versicolor', and 'Virginica'. This clearly indicates a multiclass classification task, where the goal is to predict which of these three classes a given sample belongs to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 — DATASET INTELLIGENCE\n",
    "\n",
    "### Feature Kinds:\n",
    "- **`sepal.length`, `sepal.width`, `petal.length`, `petal.width`**: These are continuous numerical features representing measurements.\n",
    "- **`variety`**: This is the target variable, a nominal categorical feature.\n",
    "\n",
    "### Missingness:\n",
    "- The dataset profile indicates 0.0% missing values across all columns. Therefore, no imputation strategies are required.\n",
    "\n",
    "### Distributions & Skew:\n",
    "- All numerical features exhibit relatively low skewness (absolute values generally below 0.35). `sepal.length` and `sepal.width` show slight positive skew, while `petal.length` and `petal.width` show slight negative skew. None of these are extreme enough to strongly suggest complex transformations like log-transforms for skew correction. Standard scaling should be sufficient for models sensitive to feature scales.\n",
    "\n",
    "### Outliers:\n",
    "- The `outlier_hint_iqr` for `sepal.width` is 0.026, which is a very small value, suggesting minimal outliers or that any 'outliers' are likely natural variations within the dataset rather than data errors. For the other numerical features, this hint is 0.0. Given the dataset's nature (Iris, often very clean), robust scaling or outlier capping is not strictly necessary but standard scaling will normalize the range.\n",
    "\n",
    "### High-Cardinality:\n",
    "- There are no high-cardinality features among the input columns. The target variable `variety` has a low cardinality of 3, which is typical for a multiclass classification target.\n",
    "\n",
    "### Leakage/ID:\n",
    "- The `identifier_like_columns` list is empty, and no features appear to be identifiers or direct proxies for the target, so no leakage is detected.\n",
    "\n",
    "### Imbalance:\n",
    "- The `categorical_top_values` for `variety` show a perfectly balanced distribution with 50 counts for each of the three classes ('Setosa', 'Versicolor', 'Virginica'). This means the dataset is perfectly balanced, which simplifies training and evaluation considerations regarding class imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 — TRANSFORMATION STRATEGY\n",
    "\n",
    "### Numerical Features (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`):\n",
    "- **Strategy**: Apply `StandardScaler`.\n",
    "- **Justification**: These are continuous numerical features. Standard scaling (mean 0, variance 1) is a common and effective preprocessing step for many machine learning models, especially distance-based models (like SVM, KNN) and models that use gradient descent (like Logistic Regression). It helps prevent features with larger ranges from dominating the learning process. Given the low skewness and minimal outlier presence, `StandardScaler` is a suitable and robust choice over `MinMaxScaler` or `RobustScaler`.\n",
    "\n",
    "### Categorical Features (None as input features; `variety` is the target):\n",
    "- **Strategy**: No explicit encoding needed for input features.\n",
    "- **Justification**: Since `variety` is the target column, it will be handled by the model's output layer (e.g., softmax for multiclass classification) and not as an input feature requiring encoding. No other categorical input features were identified.\n",
    "\n",
    "### Missing Values:\n",
    "- **Strategy**: No imputation needed.\n",
    "- **Justification**: The dataset profile shows no missing values across any column, hence no imputation strategy is required.\n",
    "\n",
    "### Target Column (`variety`):\n",
    "- **Strategy**: Mark for dropping from the feature set.\n",
    "- **Justification**: The `variety` column is the target variable for the classification task and should not be treated as an input feature during model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 — MODEL CANDIDATE SELECTION\n",
    "\n",
    "Given that this is a multiclass classification task with a relatively small and clean dataset (150 rows, 4 numerical features), a mix of interpretable and powerful models is selected.\n",
    "\n",
    "### Included Models:\n",
    "1.  **Logistic Regression**: Chosen as a strong, interpretable baseline. It's efficient, works well with scaled numerical data, and provides probabilities directly, which is useful for `log_loss` and ROC analysis. Its 'multinomial' option handles multiclass classification naturally.\n",
    "2.  **Support Vector Machine (SVC) with RBF kernel**: SVCs are powerful for both linear and non-linear classification problems and often perform well on small to medium datasets. The RBF kernel allows it to capture complex decision boundaries. It benefits significantly from scaled features.\n",
    "3.  **Random Forest Classifier**: An ensemble tree-based method known for its robustness, ability to handle interactions, and general good performance without requiring explicit feature scaling (though it doesn't hurt). It's less prone to overfitting than single decision trees and provides feature importance.\n",
    "4.  **K-Nearest Neighbors (KNeighborsClassifier)**: A simple, non-parametric, distance-based algorithm. It's highly interpretable locally and performs effectively on small datasets, especially when features are scaled appropriately. It serves as a good contrasting model to the others.\n",
    "\n",
    "### Excluded Models and Justifications:\n",
    "-   **Gradient Boosting Machines (e.g., XGBoost, LightGBM)**: While powerful, for a dataset of only 150 rows, these models can be prone to overfitting if not carefully tuned. Simpler models are often preferred for such small datasets to ensure generalization and interpretability without excessive complexity.\n",
    "-   **Neural Networks**: Typically require larger datasets to train effectively and achieve superior performance, and can be computationally more intensive to tune. For a dataset this size, their benefits are unlikely to outweigh the increased complexity and risk of overfitting compared to the selected models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4 — TRAINING & VALIDATION\n",
    "\n",
    "### Cross-Validation Strategy:\n",
    "-   **Strategy**: `StratifiedKFold` Cross-Validation (e.g., 5-folds).\n",
    "-   **Justification**: Stratified K-Fold is essential for classification tasks, especially when dealing with multiple classes. It ensures that the proportion of each class is preserved in each fold, preventing scenarios where a fold might contain very few or none of certain classes. Although the current dataset is perfectly balanced, using `StratifiedKFold` is a best practice to ensure robust validation across potential variations in class distribution if the dataset were to change or for handling future, potentially imbalanced, datasets.\n",
    "\n",
    "### Metrics:\n",
    "-   **Primary Metric**: `f1_score` (macro average).\n",
    "    -   **Justification**: F1-score is a harmonic mean of precision and recall, providing a balanced measure of a model's performance. The 'macro' average calculates metrics for each label and finds their unweighted mean, which is appropriate when all classes are equally important, as is the case with our perfectly balanced target classes.\n",
    "-   **Additional Metrics**:\n",
    "    -   `accuracy_score`: Provides a straightforward understanding of the overall correctness of predictions.\n",
    "    -   `precision_score` (macro average): Measures the proportion of true positive predictions among all positive predictions made for each class, then averaged. Important for understanding false positive rates.\n",
    "    -   `recall_score` (macro average): Measures the proportion of true positive predictions among all actual positive instances for each class, then averaged. Important for understanding false negative rates.\n",
    "    -   `log_loss`: A probabilistic metric that penalizes confident incorrect predictions. It's particularly useful for models that output probabilities and helps assess calibration and the overall quality of probabilistic forecasts.\n",
    "\n",
    "### Overfit Checks:\n",
    "-   **Method**: Monitor training performance (e.g., F1-score or accuracy on the training data) alongside validation performance (on the test folds of the cross-validation).\n",
    "-   **Justification**: If a model performs significantly better on the training data compared to the validation data across all folds, it indicates overfitting. Cross-validation inherently provides a robust estimate of generalization error, but explicitly comparing train vs. validation scores helps quickly identify models that might be memorizing the training data. For tree-based models (like Random Forest), techniques like plotting learning curves (performance vs. training set size) or examining feature importance to detect reliance on potentially noisy features can also be employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 — ERROR & BEHAVIOR ANALYSIS\n",
    "\n",
    "### 1. Confusion Matrix:\n",
    "-   **Purpose**: To visualize the performance of a classification model, providing a breakdown of correct and incorrect predictions for each class. It clearly shows which classes are being confused with each other.\n",
    "-   **Justification**: For multiclass classification, the confusion matrix is invaluable for understanding specific misclassification patterns, e.g., if 'Versicolor' is often misclassified as 'Virginica' but rarely as 'Setosa'.\n",
    "\n",
    "### 2. Classification Report:\n",
    "-   **Purpose**: Provides precision, recall, F1-score, and support for each class, along with overall averages (macro, micro, weighted).\n",
    "-   **Justification**: This offers a concise summary of per-class performance, complementing the confusion matrix by quantifying the types of errors and highlighting which classes the model struggles with in terms of false positives and false negatives.\n",
    "\n",
    "### 3. ROC AUC Curves (One-vs-Rest):\n",
    "-   **Purpose**: To evaluate the ability of a classifier to distinguish between classes across various probability thresholds. For multiclass, this is typically done using a 'one-vs-rest' approach.\n",
    "-   **Justification**: ROC AUC provides a robust metric for classifier performance that is insensitive to class imbalance (though not an issue here). Visualizing the curves per class helps assess the trade-off between true positive rate and false positive rate, especially for models outputting probabilities.\n",
    "\n",
    "### 4. Feature Importance/Coefficients:\n",
    "-   **Purpose**: To identify which features contribute most to the model's predictions.\n",
    "-   **Justification**: For models that intrinsically provide this (e.g., `RandomForestClassifier`'s `feature_importances_`, `LogisticRegression`'s `coef_`), analyzing these values helps understand the underlying decision-making process. This can reveal if the model is relying on expected features or potentially unexpected ones.\n",
    "\n",
    "### 5. Misclassified Sample Analysis:\n",
    "-   **Purpose**: To deeply inspect individual instances that the model misclassified.\n",
    "-   **Justification**: Examining specific misclassified samples can reveal patterns or common characteristics among them. This qualitative analysis might uncover data quality issues, limitations in the chosen features, or specific edge cases that the model struggles with, leading to potential data cleaning or feature engineering opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6 — EXPLAINABILITY\n",
    "\n",
    "### 1. Alignment of Post-Encoding Feature Names:\n",
    "-   **Strategy**: Since only numerical features are scaled (not encoded) and the target is not an input feature, the original feature names (`sepal.length`, `sepal.width`, `petal.length`, `petal.width`) will be directly retained post-transformation. No complex mapping from encoded features back to original features is required.\n",
    "-   **Justification**: This straightforward mapping ensures that any explanations generated will directly refer to the original, human-interpretable feature names, maintaining clarity and ease of understanding.\n",
    "\n",
    "### 2. Explainability Methods:\n",
    "-   **Permutation Importance**:\n",
    "    -   **Purpose**: A model-agnostic technique that quantifies the importance of a feature by measuring how much the model's performance decreases when that feature's values are randomly shuffled. It directly uses the original features and the model's output.\n",
    "    -   **Justification**: This method is highly reliable as it measures the true impact of a feature on a trained model's performance, without delving into the model's internal structure. It provides a global understanding of feature relevance.\n",
    "-   **SHAP (SHapley Additive exPlanations)**:\n",
    "    -   **Purpose**: Provides both local (individual prediction) and global explanations by computing the contribution of each feature to a prediction. It works by attributing the 'Shapley value' to each feature.\n",
    "    -   **Justification**: SHAP values offer a consistent and theoretically sound way to explain predictions across various models. By mapping explanations back to original feature values (which is trivial here as feature names are preserved), it provides intuitive insights into *why* a particular prediction was made for a specific instance, as well as an aggregated view of overall feature importance.\n",
    "-   **LIME (Local Interpretable Model-agnostic Explanations)**:\n",
    "    -   **Purpose**: Explains individual predictions by approximating the behavior of the black-box model locally with an interpretable model (e.g., linear model).\n",
    "    -   **Justification**: LIME is valuable for understanding local decision boundaries and building trust in specific predictions. While SHAP offers a more robust theoretical foundation, LIME's local approximations can be very intuitive for specific debugging or user communication needs.\n",
    "\n",
    "### Overall Justification:\n",
    "These explainability methods are chosen for their ability to provide both global insights into overall feature importance and local explanations for individual predictions. This dual perspective is crucial for understanding model behavior, identifying potential biases, debugging unexpected predictions, and building trust with stakeholders. The simplicity of our feature set (all numerical, no complex encoding) makes these methods straightforward to apply and interpret directly with the original feature names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For this example, we'll use the uploaded data\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['variety'])\n",
    "y = df['variety']\n",
    "\n",
    "# Handle categorical target if needed\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y.astype(str))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Preprocessing Pipeline (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing compiled from AutoMLPlan\n",
    "# Each feature transform is based on plan.feature_transforms\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "transformers = []\n",
    "\n",
    "# Create preprocessor from plan-driven transformers\n",
    "# Dropped features (from plan): ['variety']\n",
    "preprocessor = ColumnTransformer(transformers, remainder='drop')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Model (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compiled from AutoMLPlan.model_candidates\n",
    "\n",
    "# Selected model: logistic_regression (from plan.model_candidates)\n",
    "# Reason: \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Assemble Pipeline (from AutoMLPlan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble pipeline from plan-driven components\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate using metrics from AutoMLPlan\n",
    "# Metrics compiled from AutoMLPlan\n",
    "# Primary metric: f1_score_macro\n",
    "# Additional metrics: ['accuracy_score', 'precision_score_macro', 'recall_score_macro', 'log_loss']\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "# Additional metrics from plan:\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (from plan.explainability_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance (aligned with plan)\n",
    "if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    importances = pipeline.named_steps['model'].feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing (aligned with plan, not dtype-based)\n",
    "    try:\n",
    "        preprocessor = pipeline.named_steps['preprocessor']\n",
    "        feature_names = []\n",
    "        for name, transformer, cols in preprocessor.transformers_:\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                feature_names.extend(transformer.get_feature_names_out(cols))\n",
    "            elif hasattr(transformer, 'named_steps'):\n",
    "                for step_name, step_transformer in transformer.named_steps.items():\n",
    "                    if hasattr(step_transformer, 'get_feature_names_out'):\n",
    "                        feature_names.extend(step_transformer.get_feature_names_out(cols))\n",
    "                        break\n",
    "            else:\n",
    "                feature_names.extend([f'{name}_{col}' for col in cols])\n",
    "    except:\n",
    "        feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names[:len(importances)],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(importance_df)\n",
    "else:\n",
    "    print('Feature importance not available for this model type.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Save label encoder if used\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for predictions\n",
    "# with open('model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "\n",
    "# Example prediction\n",
    "# new_data = pd.DataFrame({...})\n",
    "# prediction = loaded_model.predict(new_data)\n",
    "# print(f'Prediction: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
