{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intent2Model Research Experiments\n",
        "\n",
        "This notebook contains experiments and analysis for the Intent2Model system.\n",
        "\n",
        "## Research Claims\n",
        "\n",
        "1. **Usability**: Improves ML usability for non-experts\n",
        "2. **Safety**: Reduces incorrect pipeline configuration\n",
        "3. **Intent Alignment**: Aligns evaluation metric with real user intent\n",
        "\n",
        "Focus is NOT raw accuracy, but intent alignment and safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add backend to path\n",
        "sys.path.insert(0, str(Path().parent.parent / \"backend\"))\n",
        "\n",
        "from research.user_simulation import simulate_user_interaction, run_simulation_experiment\n",
        "from research.ablation_tests import run_ablation_suite, AblationConfig, analyze_ablation_results\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. User Simulation Experiments\n",
        "\n",
        "Simulate different user types (novice, intermediate, expert) and measure:\n",
        "- Number of questions asked\n",
        "- Mistakes detected and corrected\n",
        "- Success rate\n",
        "- Performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create sample datasets for testing\n",
        "# In practice, load real datasets\n",
        "\n",
        "def create_sample_classification_dataset():\n",
        "    \"\"\"Create a sample classification dataset.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = 200\n",
        "    X1 = np.random.randn(n_samples)\n",
        "    X2 = np.random.randn(n_samples)\n",
        "    X3 = np.random.choice(['A', 'B', 'C'], n_samples)\n",
        "    y = (X1 + X2 > 0).astype(int)\n",
        "    df = pd.DataFrame({\n",
        "        'feature1': X1,\n",
        "        'feature2': X2,\n",
        "        'feature3': X3,\n",
        "        'target': y\n",
        "    })\n",
        "    return df, 'target', 'classification'\n",
        "\n",
        "def create_sample_regression_dataset():\n",
        "    \"\"\"Create a sample regression dataset.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n_samples = 200\n",
        "    X1 = np.random.randn(n_samples)\n",
        "    X2 = np.random.randn(n_samples)\n",
        "    X3 = np.random.choice(['A', 'B', 'C'], n_samples)\n",
        "    y = 2 * X1 + 3 * X2 + np.random.randn(n_samples) * 0.5\n",
        "    df = pd.DataFrame({\n",
        "        'feature1': X1,\n",
        "        'feature2': X2,\n",
        "        'feature3': X3,\n",
        "        'target': y\n",
        "    })\n",
        "    return df, 'target', 'regression'\n",
        "\n",
        "# Run simulation experiment\n",
        "datasets = [\n",
        "    create_sample_classification_dataset(),\n",
        "    create_sample_regression_dataset()\n",
        "]\n",
        "\n",
        "# Note: This may take a while if using LLM\n",
        "# simulation_results = run_simulation_experiment(\n",
        "#     datasets, \n",
        "#     user_types=[\"novice\", \"intermediate\", \"expert\"],\n",
        "#     n_runs=5\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ablation Tests\n",
        "\n",
        "Test the impact of each component:\n",
        "- Questioning agent\n",
        "- Evaluator warnings\n",
        "- LLM planning\n",
        "- Explainer agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ablation tests\n",
        "# ablation_results = run_ablation_suite(datasets)\n",
        "\n",
        "# Analyze results\n",
        "# analysis = analyze_ablation_results(ablation_results)\n",
        "# print(\"Ablation Analysis:\")\n",
        "# print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualization\n",
        "\n",
        "Plot success rates, time to model, and metric alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example plotting functions (uncomment when you have results)\n",
        "\n",
        "# def plot_success_rate_by_user_type(results_df):\n",
        "#     \"\"\"Plot success rate by user type.\"\"\"\n",
        "#     success_by_type = results_df.groupby('user_type')['success'].mean()\n",
        "#     success_by_type.plot(kind='bar', title='Success Rate by User Type')\n",
        "#     plt.ylabel('Success Rate')\n",
        "#     plt.xlabel('User Type')\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_questions_asked(results_df):\n",
        "#     \"\"\"Plot distribution of questions asked.\"\"\"\n",
        "#     results_df['questions_asked'].hist(bins=10, title='Distribution of Questions Asked')\n",
        "#     plt.xlabel('Number of Questions')\n",
        "#     plt.ylabel('Frequency')\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_ablation_comparison(ablation_results):\n",
        "#     \"\"\"Compare ablation configurations.\"\"\"\n",
        "#     ablation_groups = ablation_results.groupby('ablation_id')\n",
        "#     success_rates = ablation_groups['training_success'].mean()\n",
        "#     success_rates.plot(kind='bar', title='Training Success Rate by Configuration')\n",
        "#     plt.ylabel('Success Rate')\n",
        "#     plt.xlabel('Ablation Configuration')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Metrics Analysis\n",
        "\n",
        "Key metrics to track:\n",
        "- **Success Rate**: Percentage of successful pipeline configurations\n",
        "- **Time to Model**: Time from dataset upload to trained model\n",
        "- **Metric Alignment**: Whether chosen metric matches user intent\n",
        "- **Mistake Recovery**: Ability to detect and correct user mistakes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example metrics calculation (uncomment when you have results)\n",
        "\n",
        "# def calculate_metrics(results_df):\n",
        "#     \"\"\"Calculate key research metrics.\"\"\"\n",
        "#     metrics = {\n",
        "#         'overall_success_rate': results_df['success'].mean(),\n",
        "#         'avg_questions': results_df['questions_asked'].mean(),\n",
        "#         'mistake_recovery_rate': ...  # Calculate based on mistakes_made vs success\n",
        "#     }\n",
        "#     return metrics\n",
        "\n",
        "# metrics = calculate_metrics(simulation_results)\n",
        "# print(\"Research Metrics:\")\n",
        "# for key, value in metrics.items():\n",
        "#     print(f\"{key}: {value:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
